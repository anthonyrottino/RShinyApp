import numpy as np
import pandas as pd
import streamlit as st

from sklearn.decomposition import PCA
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error
from lightgbm import LGBMRegressor

import shap
import matplotlib.pyplot as plt


# ============================================================
#                 1) PCA SUR LES RETURNS DU BASIS
# ============================================================

def compute_pca_returns(df_train, basis_cols, n_components=2):
    """Fit PCA on return matrix of basis curves (train uniquement)."""

    R = df_train[basis_cols].diff(1).fillna(0.0)

    pca = PCA(n_components=n_components, random_state=42)
    scores = pca.fit_transform(R.values)

    pcs_train = pd.DataFrame(
        scores,
        index=df_train.index,
        columns=[f"PC{i+1}_basis" for i in range(n_components)]
    )
    return pca, pcs_train


def apply_pca_to_full_df(pca, df, basis_cols):
    R_full = df[basis_cols].diff(1).fillna(0.0)
    scores_full = pca.transform(R_full.values)

    pcs_full = pd.DataFrame(
        scores_full,
        index=df.index,
        columns=[f"PC{i+1}_basis" for i in range(scores_full.shape[1])]
    )
    return pcs_full


# ============================================================
#                 2) FEATURE ENGINEERING
# ============================================================

def build_features_for_tenor(df, tenor):

    target = f"EUBASIS_{tenor}"
    usd = f"USDSWAP_{tenor}"
    eur = f"EURSWAP_{tenor}"

    if target not in df.columns:
        raise ValueError(f"{target} missing in dataframe")

    X = df.copy()

    # Swaps spread & slopes
    X["spread_T"] = X[usd] - X[eur]
    X["slope_usd"] = X["USDSWAP_10Y"] - X["USDSWAP_2Y"]
    X["slope_eur"] = X["EURSWAP_10Y"] - X["EURSWAP_2Y"]

    # Returns (1d, 5d)
    X["d_usd_1d"] = X[usd].diff(1).fillna(0.0)
    X["d_eur_1d"] = X[eur].diff(1).fillna(0.0)
    X["d_basis_1d"] = X[target].diff(1).fillna(0.0)

    X["d_usd_5d"] = X[usd].diff(5).fillna(0.0)
    X["d_eur_5d"] = X[eur].diff(5).fillna(0.0)
    X["d_basis_5d"] = X[target].diff(5).fillna(0.0)

    # Volatilities
    ret_basis = X[target].diff(1).fillna(0.0)
    X["vol_20d"] = ret_basis.rolling(20).std().fillna(0.0)
    X["vol_60d"] = ret_basis.rolling(60).std().fillna(0.0)

    # FX returns
    X["d_EURUSD_1d"] = X["EURUSD"].diff().fillna(0.0)

    # VIX (optional)
    if "VIX" not in X.columns:
        X["VIX"] = 0.0
    X["d_VIX_1d"] = X["VIX"].diff().fillna(0.0)
    X["vol_VIX_20d"] = X["VIX"].diff().rolling(20).std().fillna(0.0)

    # Momentum
    X["mom_5_20"] = (X[target].diff(5) / X[target].diff(20)).replace([np.inf, -np.inf], 0).fillna(0.0)

    # FULL LIST OF FEATURES (including PCA!)
    FEATURES = [
        usd, eur, "spread_T",
        "slope_usd", "slope_eur",
        "d_usd_1d", "d_eur_1d", "d_basis_1d",
        "d_usd_5d", "d_eur_5d", "d_basis_5d",
        "vol_20d", "vol_60d",
        "PC1_basis", "PC2_basis",        # <<<<<< INDISPENSABLE !
        "EURUSD", "d_EURUSD_1d",
        "VIX", "d_VIX_1d", "vol_VIX_20d",
        "mom_5_20",
    ]

    out = X[FEATURES + [target]].rename(columns={target: "target_basis"})
    out = out.dropna(subset=["target_basis"])
    out = out.fillna(0.0)

    return out, FEATURES


# ============================================================
#                 3) LGBM + GRIDSEARCH + SHAP
# ============================================================

def run_gridsearch(X, y):

    params = {
        "learning_rate": [0.01, 0.03],
        "num_leaves": [31, 63],
        "max_depth": [-1, 8],
        "n_estimators": [300, 600]
    }

    base = LGBMRegressor(
        subsample=0.9,
        colsample_bytree=0.9,
        min_child_samples=20,
        random_state=42,
        n_jobs=-1
    )

    cv = TimeSeriesSplit(n_splits=3)

    grid = GridSearchCV(
        base,
        params,
        scoring="neg_root_mean_squared_error",
        cv=cv,
        n_jobs=-1,
    )

    grid.fit(X, y)
    df_cv = pd.DataFrame(grid.cv_results_)
    df_cv["RMSE"] = -df_cv["mean_test_score"]

    return grid.best_params_, df_cv.sort_values("RMSE")


def train_lgbm_with_shap(X, y):

    best_params, cv_table = run_gridsearch(X, y)

    model = LGBMRegressor(**best_params, random_state=42, n_jobs=-1)
    model.fit(X, y)

    # SHAP
    explainer = shap.TreeExplainer(model)
    Xs = X.sample(min(800, len(X)), random_state=42)
    shap_vals = explainer.shap_values(Xs)

    importance = pd.Series(
        np.abs(shap_vals).mean(axis=0),
        index=Xs.columns
    ).sort_values(ascending=False)

    # Keep 95% of mass
    keep = []
    total = importance.sum()
    c = 0
    for f, v in importance.items():
        keep.append(f)
        c += v
        if c / total >= 0.95:
            break

    # Retrain final model
    final = LGBMRegressor(**best_params, random_state=42, n_jobs=-1)
    final.fit(X[keep], y)

    return final, keep, importance, best_params, cv_table


# ============================================================
#                 4) STREAMLIT APPLICATION
# ============================================================

def main():

    st.title("ðŸ“ˆ EURUSD Basis Fair Value â€“ PCA Returns + LGBM + SHAP")

    up = st.file_uploader("Upload CSV", type=["csv"])
    if up is None:
        st.stop()

    df = pd.read_csv(up)
    df["date"] = pd.to_datetime(df["date"], format="%d/%m/%Y")
    df = df.set_index("date").sort_index()

    # Identify basis curves
    basis_cols = [c for c in df.columns if c.startswith("EUBASIS_")]
    tenors = [c.replace("EUBASIS_", "") for c in basis_cols]

    # 80/20 temporal split
    split = st.slider("Train %", 60, 95, 80)
    N = len(df)
    k = int(N * split / 100)

    df_train = df.iloc[:k]
    df_test = df.iloc[k:]

    st.info(
        f"Train: {df_train.index[0].date()} â†’ {df_train.index[-1].date()}\n"
        f"Test : {df_test.index[0].date()} â†’ {df_test.index[-1].date()}"
    )

    # ========== PCA RETURNS ==========
    pca, pcs_train = compute_pca_returns(df_train, basis_cols)
    pcs_full = apply_pca_to_full_df(pca, df, basis_cols)

    df = df.join(pcs_full, how="left")

    sel = st.multiselect("Tenors", tenors, default=["1Y", "2Y", "5Y"])

    if st.button("Run Calibration"):
        st.session_state["run"] = True

    if "run" not in st.session_state or not st.session_state["run"]:
        st.stop()

    # ========== MAIN LOOP ==========
    results = {}
    best_params_all = {}
    features_all = {}

    for tenor in sel:

        st.subheader(f"Calibration {tenor}")

        try:
            feat_all, FEATURES = build_features_for_tenor(df, tenor)
        except Exception as e:
            st.error(f"{tenor} skipped: {e}")
            continue

        idx_train = feat_all.index.intersection(df_train.index)
        X_train = feat_all.loc[idx_train].drop(columns=["target_basis"])
        y_train = feat_all.loc[idx_train]["target_basis"]

        model, keep, shap_imp, best_params, cv_table = train_lgbm_with_shap(X_train, y_train)

        best_params_all[tenor] = best_params
        features_all[tenor] = keep

        # Full prediction
        X_all = feat_all[keep]
        y_all = feat_all["target_basis"]
        y_hat = model.predict(X_all)

        results[tenor] = pd.DataFrame({"basis": y_all, "fair": y_hat}, index=feat_all.index)

    # ============================================================
    #              DISPLAY RESULTS (graphs & tables)
    # ============================================================

    st.header("ðŸ“Š Basis vs Fair Value")

    tenor_disp = st.selectbox("Tenor to display", list(results.keys()))
    dfp = results[tenor_disp]

    idx_is = dfp.index.intersection(df_train.index)
    idx_os = dfp.index.intersection(df_test.index)

    df_is = dfp.loc[idx_is]
    df_os = dfp.loc[idx_os]

    def metr(a, b):
        return {
            "RMSE": np.sqrt(mean_squared_error(a, b)),
            "MAE": mean_absolute_error(a, b),
            "IC": np.corrcoef(a, b)[0,1]
        }

    st.subheader("Metrics")
    st.dataframe(pd.DataFrame({
        "In-sample": metr(df_is["basis"], df_is["fair"]),
        "Out-sample": metr(df_os["basis"], df_os["fair"])
    }))

    fig, ax = plt.subplots(figsize=(10,4))
    ax.plot(dfp.index, dfp["basis"], label="Basis")
    ax.plot(dfp.index, dfp["fair"], label="Fair Value", linestyle="--")
    ax.axvspan(df_test.index.min(), df_test.index.max(), color="lightgray", alpha=0.3)
    ax.set_title(f"{tenor_disp}")
    ax.grid(True)
    ax.legend()
    st.pyplot(fig)

    st.subheader("Best hyperparameters")
    st.dataframe(pd.DataFrame(best_params_all).T)

    st.subheader("Features kept (SHAP)")
    all_feat = sorted({f for fl in features_all.values() for f in fl})
    mat = pd.DataFrame(False, index=all_feat, columns=features_all.keys())
    for t, cols in features_all.items():
        mat.loc[cols, t] = True
    st.dataframe(mat)


if __name__ == "__main__":
    main()
