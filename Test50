import numpy as np
import pandas as pd
import streamlit as st

from sklearn.decomposition import PCA
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error
from lightgbm import LGBMRegressor

import shap
import matplotlib.pyplot as plt


# ============================================================
#                   PCA sur RETURNS du BASIS
# ============================================================

@st.cache_data
def fit_pca_on_basis_returns(df_train, basis_cols):
    ret_train = df_train[basis_cols].diff(1).fillna(0.0)

    pca = PCA(n_components=2, random_state=42)
    scores_train = pca.fit_transform(ret_train.values)

    pcs_train = pd.DataFrame(
        scores_train,
        index=df_train.index,
        columns=["PC1_basis", "PC2_basis"],
    )
    return pca, pcs_train


def apply_pca_returns_full(pca, df, basis_cols):
    ret_full = df[basis_cols].diff(1).fillna(0.0)

    scores_full = pca.transform(ret_full.values)

    pcs_full = pd.DataFrame(
        scores_full,
        index=df.index,
        columns=["PC1_basis", "PC2_basis"],
    )
    return pcs_full


# ============================================================
#                   FEATURE BUILDER (stable)
# ============================================================

def build_features_for_tenor(df, tenor: str):
    target_col = f"EUBASIS_{tenor}"

    usd_col = f"USDSWAP_{tenor}"
    eur_col = f"EURSWAP_{tenor}"

    if target_col not in df.columns:
        raise ValueError(f"{target_col} missing")

    data = df.copy()

    # Spread et slopes
    data["spread_T"] = data[usd_col] - data[eur_col]
    data["slope_usd_10y_2y"] = data["USDSWAP_10Y"] - data["USDSWAP_2Y"]
    data["slope_eur_10y_2y"] = data["EURSWAP_10Y"] - data["EURSWAP_2Y"]

    # Returns USD/EUR/Basis 1d et 5d
    data["d_usd_1d"]   = data[usd_col].diff(1).fillna(0.0)
    data["d_eur_1d"]   = data[eur_col].diff(1).fillna(0.0)
    data["d_basis_1d"] = data[target_col].diff(1).fillna(0.0)

    data["d_usd_5d"]   = data[usd_col].diff(5).fillna(0.0)
    data["d_eur_5d"]   = data[eur_col].diff(5).fillna(0.0)
    data["d_basis_5d"] = data[target_col].diff(5).fillna(0.0)

    # Vol de returns du basis
    ret_basis = data[target_col].diff(1).fillna(0.0)
    data["vol_basis_20d"] = ret_basis.rolling(20).std().fillna(0.0)
    data["vol_basis_60d"] = ret_basis.rolling(60).std().fillna(0.0)

    # FX + returns FX
    data["d_EURUSD_1d"] = data["EURUSD"].diff(1).fillna(0.0)

    # VIX + returns VIX
    if "VIX" not in data.columns:
        data["VIX"] = 0.0
        data["d_VIX_1d"] = 0.0
        data["vol_VIX_20d"] = 0.0
    else:
        data["d_VIX_1d"] = data["VIX"].diff(1).fillna(0.0)
        data["vol_VIX_20d"] = data["VIX"].diff(1).rolling(20).std().fillna(0.0)

    # Momentum basis
    data["mom_basis_5_20"] = (
        data[target_col].diff(5) / data[target_col].diff(20)
    ).replace([np.inf, -np.inf], 0.0).fillna(0.0)

    FEATURES = [
        usd_col, eur_col, "spread_T",
        "slope_usd_10y_2y", "slope_eur_10y_2y",
        "d_usd_1d", "d_eur_1d", "d_basis_1d",
        "d_usd_5d", "d_eur_5d", "d_basis_5d",
        "vol_basis_20d", "vol_basis_60d",
        "PC1_basis", "PC2_basis",
        "EURUSD", "d_EURUSD_1d",
        "VIX", "d_VIX_1d", "vol_VIX_20d",
        "mom_basis_5_20",
    ]

    out = data[FEATURES + [target_col]].rename(columns={target_col: "target_basis"})

    out = out.dropna(subset=["target_basis"])      # seule ligne autorisÃ©e
    feat_cols = [c for c in out.columns if c != "target_basis"]
    out[feat_cols] = out[feat_cols].fillna(0.0)

    return out


# ============================================================
#                   LGBM + GRID SEARCH (RMSE)
# ============================================================

def tune_lgbm(X, y):
    param_grid = {
        "learning_rate": [0.01, 0.03],
        "num_leaves": [31, 63],
        "max_depth": [-1, 8],
        "n_estimators": [300, 600],
    }

    base = LGBMRegressor(
        subsample=0.9,
        colsample_bytree=0.9,
        min_child_samples=20,
        random_state=42,
        n_jobs=-1,
    )

    cv = TimeSeriesSplit(n_splits=3)

    grid = GridSearchCV(
        estimator=base,
        param_grid=param_grid,
        scoring="neg_root_mean_squared_error",
        cv=cv,
        n_jobs=-1,
        verbose=0,
        return_train_score=False,
    )

    grid.fit(X, y)

    res = pd.DataFrame(grid.cv_results_)
    res["RMSE"] = -res["mean_test_score"]
    res = res.sort_values("RMSE").reset_index(drop=True)

    best = grid.best_params_
    best["subsample"] = 0.9
    best["colsample_bytree"] = 0.9
    best["min_child_samples"] = 20

    return best, res


def train_lgbm_and_shap(X, y):
    best_params, cv_table = tune_lgbm(X, y)

    model_full = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
    )
    model_full.fit(X, y)

    explainer = shap.TreeExplainer(model_full)
    X_shap = X.sample(min(800, len(X)), random_state=42)
    shap_vals = explainer.shap_values(X_shap)

    shap_abs = np.abs(shap_vals).mean(axis=0)
    shap_importances = pd.Series(shap_abs, index=X.columns).sort_values(ascending=False)

    total = shap_importances.sum()
    cum = 0
    keep_cols = []
    for col, val in shap_importances.items():
        keep_cols.append(col)
        cum += val
        if cum / total >= 0.95:
            break

    model_final = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
    )

    model_final.fit(X[keep_cols], y)

    return model_final, keep_cols, shap_importances, best_params, cv_table


@st.cache_resource
def cached_lgbm_train(X, y):
    return train_lgbm_and_shap(X, y)


# ============================================================
#                       STREAMLIT APP
# ============================================================

def main():

    st.title("ðŸ“ˆ EURUSD Basis Fair Value â€” PCA Returns + LGBM + SHAP")

    up = st.file_uploader("ðŸ“„ Upload CSV", type=["csv"])
    if up is None:
        st.stop()

    df = pd.read_csv(up)
    df["date"] = pd.to_datetime(df["date"], format="%d/%m/%Y")
    df = df.set_index("date").sort_index()

    basis_cols = [c for c in df.columns if c.startswith("EUBASIS_")]
    if not basis_cols:
        st.error("Aucune colonne EUBASIS_ trouvÃ©e !")
        st.stop()

    # ---------- split 80/20 ----------
    split_pct = st.slider("Pourcentage In-Sample", 60, 95, 80)
    N = len(df)
    k = int(N * split_pct / 100)
    df_train = df.iloc[:k].copy()
    df_test = df.iloc[k:].copy()

    st.info(
        f"Train: {df_train.index[0].date()} â†’ {df_train.index[-1].date()}\n"
        f"Test : {df_test.index[0].date()} â†’ {df_test.index[-1].date()}"
    )

    # ---------- PCA RETURNS ----------
    pca_ret, pcs_train = fit_pca_on_basis_returns(df_train, basis_cols)
    pcs_full = apply_pca_returns_full(pca_ret, df, basis_cols)

    df = df.join(pcs_full, how="left")

    tenors = sorted([c.replace("EUBASIS_", "") for c in basis_cols])
    sel = st.multiselect("Tenors", tenors, default=["1Y", "2Y", "5Y"])

    if not sel:
        st.stop()

    if st.button("ðŸš€ Run Calibration"):
        st.session_state["run"] = True

    if "run" not in st.session_state or not st.session_state["run"]:
        st.stop()

    status = st.empty()
    progress = st.progress(0.0)

    results = {}
    features_all = {}
    best_params_all = {}

    # ============================================================
    #                      TRAIN TENOR BY TENOR
    # ============================================================

    for i, tenor in enumerate(sel, start=1):
        status.markdown(f"**Calibration {tenor} ({i}/{len(sel)})â€¦**")
        progress.progress(i / len(sel))

        try:
            feat_all = build_features_for_tenor(df, tenor)
        except Exception as e:
            st.warning(f"{tenor} ignorÃ© : {e}")
            continue

        idx_train = feat_all.index.intersection(df_train.index)
        feat_train = feat_all.loc[idx_train]

        if feat_train.shape[0] < 80:
            st.warning(f"{tenor} : pas assez de train.")
            continue

        X_tr = feat_train.drop(columns=["target_basis"])
        y_tr = feat_train["target_basis"]

        try:
            model, keep_cols, shap_imp, best_params, cv_tab = cached_lgbm_train(X_tr, y_tr)
        except Exception as e:
            st.warning(f"{tenor} : erreur modÃ¨le â†’ {e}")
            continue

        best_params_all[tenor] = best_params
        features_all[tenor] = keep_cols

        # full prediction
        X_all = feat_all[keep_cols]
        y_all = feat_all["target_basis"]

        y_hat = model.predict(X_all)

        df_res = pd.DataFrame({"basis": y_all, "fair": y_hat}, index=feat_all.index)
        results[tenor] = df_res

    # ============================================================
    #                      VISUALISATION
    # ============================================================

    if not results:
        st.error("Aucun tenor calibrÃ©.")
        st.stop()

    st.header("ðŸ“ˆ Basis vs Fair Value â€” Train & Test")

    tenor = st.selectbox("Tenor", list(results.keys()))
    dfp = results[tenor]

    idx_is = dfp.index.intersection(df_train.index)
    idx_os = dfp.index.intersection(df_test.index)

    df_is = dfp.loc[idx_is]
    df_os = dfp.loc[idx_os]

    def metrics(a, b):
        return {
            "RMSE": np.sqrt(mean_squared_error(a, b)),
            "MAE": mean_absolute_error(a, b),
            "IC": np.corrcoef(a, b)[0, 1],
        }

    st.subheader("MÃ©triques In/Out-sample")
    met_is = metrics(df_is["basis"], df_is["fair"])
    met_os = metrics(df_os["basis"], df_os["fair"])

    st.dataframe(pd.DataFrame({"In-sample": met_is, "Out-sample": met_os}))

    fig, ax = plt.subplots(figsize=(10,4))
    ax.plot(dfp.index, dfp["basis"], label="Basis")
    ax.plot(dfp.index, dfp["fair"], label="Fair Value", linestyle="--")

    ax.axvspan(
        df_test.index.min(),
        df_test.index.max(),
        color="lightgray", alpha=0.3, label="TEST"
    )

    ax.set_title(f"Basis vs Fair Value â€“ {tenor}")
    ax.grid(True)
    ax.legend()
    st.pyplot(fig)

    # ============================================================
    #                     TABLEAUX GLOBALS
    # ============================================================

    st.header("ðŸ“Š Hyperparams & Features")

    st.subheader("Best hyperparams")
    st.dataframe(pd.DataFrame(best_params_all).T)

    st.subheader("Features retenues (SHAP)")
    all_feats = sorted({f for cols in features_all.values() for f in cols})
    mat = pd.DataFrame(False, index=all_feats, columns=features_all.keys())
    for t, cols in features_all.items():
        mat.loc[cols, t] = True
    st.dataframe(mat)


if __name__ == "__main__":
    main()
