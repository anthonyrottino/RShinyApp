import numpy as np
import pandas as pd
import streamlit as st

from sklearn.decomposition import PCA
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error

from lightgbm import LGBMRegressor
import shap
import matplotlib.pyplot as plt


# ============================================================
#                 PCA HELPERS
# ============================================================

def fit_pca_on_basis(df, basis_cols, n_components=2):
    """Fit PCA sur la courbe de basis."""
    X = df[basis_cols].dropna()
    pca = PCA(n_components=n_components, random_state=42)
    scores = pca.fit_transform(X.values)
    pcs = pd.DataFrame(
        scores,
        index=X.index,
        columns=[f"PC{i+1}_basis" for i in range(n_components)],
    )
    return pca, pcs


def apply_pca(pca, df, basis_cols):
    """Applique le PCA sur tout le dataset."""
    X = df[basis_cols].copy()
    mask = X.notna().all(axis=1)
    scores = pca.transform(X.loc[mask])
    pcs = pd.DataFrame(
        scores,
        index=X.loc[mask].index,
        columns=[f"PC{i+1}_basis" for i in range(pca.n_components_)],
    )
    return pcs


@st.cache_data
def cached_pca(df_train, basis_cols):
    return fit_pca_on_basis(df_train, basis_cols, n_components=2)


# ============================================================
#                 FEATURE BUILDER
# ============================================================

def build_features_for_tenor(df, tenor: str) -> pd.DataFrame:
    """
    Construit les features pour un tenor donnÃ© :
      - USDSWAP_T, EURSWAP_T, spread_T
      - slopes 10Yâ€“2Y (USD/EUR)
      - PC1_basis, PC2_basis
      - EURUSD
      - d_basis_5d, vol_basis_60d
    """
    target_col = f"EUBASIS_{tenor}"
    if target_col not in df.columns:
        raise ValueError(f"Colonne {target_col} manquante dans le CSV.")

    usd_col = f"USDSWAP_{tenor}"
    eur_col = f"EURSWAP_{tenor}"
    if usd_col not in df.columns or eur_col not in df.columns:
        raise ValueError(f"Colonnes {usd_col} ou {eur_col} manquantes dans le CSV.")

    data = df.copy()

    # Spread local
    data["spread_T"] = data[usd_col] - data[eur_col]

    # Slopes 10Yâ€“2Y (on suppose que ces colonnes existent)
    required_slopes = ["USDSWAP_10Y", "USDSWAP_2Y", "EURSWAP_10Y", "EURSWAP_2Y"]
    for col in required_slopes:
        if col not in data.columns:
            raise ValueError(f"Colonne {col} manquante pour calculer les slopes 10Yâ€“2Y.")

    data["slope_usd_10y_2y"] = data["USDSWAP_10Y"] - data["USDSWAP_2Y"]
    data["slope_eur_10y_2y"] = data["EURSWAP_10Y"] - data["EURSWAP_2Y"]

    # Dynamiques du basis
    data["d_basis_5d"] = data[target_col].diff(5)
    data["vol_basis_60d"] = data[target_col].rolling(60).std()

    # PCA factors
    for col in ["PC1_basis", "PC2_basis"]:
        if col not in data.columns:
            raise ValueError(f"{col} manquant dans le DataFrame (PCA).")

    # FX
    if "EURUSD" not in data.columns:
        raise ValueError("Colonne EURUSD manquante dans le CSV.")

    FEATURES = [
        usd_col,
        eur_col,
        "spread_T",
        "slope_usd_10y_2y",
        "slope_eur_10y_2y",
        "PC1_basis",
        "PC2_basis",
        "EURUSD",
        "d_basis_5d",
        "vol_basis_60d",
    ]

    out = data[FEATURES + [target_col]].dropna()
    out = out.rename(columns={target_col: "target_basis"})
    return out


# ============================================================
#            LGBM + GRID SEARCH (LIGHT) + SHAP
# ============================================================

def tune_lgbm_hyperparams(X: pd.DataFrame, y: pd.Series):
    """
    GridSearch "light" sur les 4 paramÃ¨tres principaux :
      - learning_rate
      - num_leaves
      - max_depth
      - n_estimators
    2x2x2x2 = 16 combinaisons â†’ trÃ¨s rapide.
    """
    if X.shape[0] < 100:
        st.warning("Pas assez de donnÃ©es pour un GridSearch sÃ©rieux, on utilise des paramÃ¨tres par dÃ©faut.")
        default_params = {
            "learning_rate": 0.03,
            "num_leaves": 63,
            "max_depth": -1,
            "n_estimators": 400,
            "subsample": 0.9,
            "colsample_bytree": 0.9,
            "min_child_samples": 20,
        }
        return default_params, pd.DataFrame()

    param_grid = {
        "learning_rate": [0.01, 0.03],
        "num_leaves": [31, 63],
        "max_depth": [-1, 8],
        "n_estimators": [300, 600],
    }

    base_model = LGBMRegressor(
        subsample=0.9,
        colsample_bytree=0.9,
        min_child_samples=20,
        random_state=42,
        n_jobs=-1,
    )

    tscv = TimeSeriesSplit(n_splits=2)

    grid = GridSearchCV(
        estimator=base_model,
        param_grid=param_grid,
        cv=tscv,
        scoring="neg_mean_squared_error",
        n_jobs=-1,
        verbose=0,
        return_train_score=True,
    )

    grid.fit(X, y)

    cv_results = pd.DataFrame(grid.cv_results_)
    cv_results["MSE"] = -cv_results["mean_test_score"]
    cv_results = cv_results.sort_values("MSE").reset_index(drop=True)

    best_params = grid.best_params_.copy()
    best_params["subsample"] = 0.9
    best_params["colsample_bytree"] = 0.9
    best_params["min_child_samples"] = 20

    # SÃ©curitÃ© : ne JAMAIS laisser n_jobs traÃ®ner dans best_params
    if "n_jobs" in best_params:
        best_params.pop("n_jobs")

    return best_params, cv_results


def train_lgbm_and_shap(X: pd.DataFrame, y: pd.Series):
    """
    Pipeline :
      1) GridSearch hyperparams
      2) LGBM FULL (toutes features) pour SHAP
      3) SHAP sur un Ã©chantillon (max 800 points)
      4) SÃ©lection de features (95% masse SHAP)
      5) LGBM FINAL rÃ©-entraÃ®nÃ© sur les features sÃ©lectionnÃ©es
    """
    if X.shape[0] < 50:
        raise ValueError("Pas assez de points pour LGBM + SHAP.")

    # 1) Hyperparams
    best_params, cv_full = tune_lgbm_hyperparams(X, y)

    # 2) ModÃ¨le FULL
    if "n_jobs" in best_params:
        best_params.pop("n_jobs")

    model_full = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
    )
    model_full.fit(X, y)

    # 3) SHAP sur un sample
    explainer = shap.TreeExplainer(model_full)
    X_shap = X.sample(min(800, len(X)), random_state=42)
    shap_values = explainer.shap_values(X_shap)

    shap_abs = np.abs(shap_values).mean(axis=0)
    shap_importances = pd.Series(shap_abs, index=X.columns).sort_values(ascending=False)

    # 4) SÃ©lection features jusqu'Ã  95% masse SHAP
    total = shap_importances.sum()
    cum = 0.0
    keep_cols = []
    for col, val in shap_importances.items():
        keep_cols.append(col)
        cum += val
        if cum / total >= 0.95:
            break

    # 5) ModÃ¨le FINAL sur keep_cols uniquement
    if "n_jobs" in best_params:
        best_params.pop("n_jobs")

    model_final = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
    )
    model_final.fit(X[keep_cols], y)

    return model_final, keep_cols, shap_importances, best_params, cv_full


@st.cache_resource
def cached_lgbm_train(X: pd.DataFrame, y: pd.Series):
    """Cache le training LGBM+Grid+SHAP pour (X,y)."""
    return train_lgbm_and_shap(X, y)


# ============================================================
#                    STREAMLIT APP
# ============================================================

def main():
    st.title("ðŸ“ˆ EURUSD Basis â€“ Fair Value Calibration (LGBM + PCA + SHAP)")

    uploaded = st.file_uploader("ðŸ“„ Upload main CSV (multi-asset)", type=["csv"])
    if uploaded is None:
        st.stop()

    # --- Lecture
    df = pd.read_csv(uploaded)
    if "date" not in df.columns:
        st.error("Le CSV doit contenir une colonne 'date'.")
        st.stop()

    df["date"] = pd.to_datetime(df["date"], format="%d/%m/%Y")
    df = df.set_index("date").sort_index()

    # --- Basis columns
    basis_cols = [c for c in df.columns if c.startswith("EUBASIS_")]
    if not basis_cols:
        st.error("Aucune colonne EUBASIS_ trouvÃ©e dans le CSV.")
        st.stop()

    # --- Split temporel
    st.markdown("### ðŸ”€ Train / Test Split")
    split_pct = st.slider("Pourcentage In-sample", min_value=60, max_value=95, value=80)

    N = len(df)
    split_idx = int(N * split_pct / 100)
    split_idx = max(1, min(split_idx, N - 1))

    df_train = df.iloc[:split_idx].copy()
    df_test = df.iloc[split_idx:].copy()

    st.info(
        f
