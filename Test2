import numpy as np
import pandas as pd
import streamlit as st

from sklearn.decomposition import PCA
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error

from lightgbm import LGBMRegressor
import shap
import matplotlib.pyplot as plt


# ============================================================
#                 PCA HELPERS
# ============================================================

def fit_pca_on_basis(df, basis_cols, n_components=2):
    """Fit PCA sur la courbe de basis."""
    X = df[basis_cols].dropna()
    pca = PCA(n_components=n_components, random_state=42)
    scores = pca.fit_transform(X.values)
    pcs = pd.DataFrame(
        scores,
        index=X.index,
        columns=[f"PC{i+1}_basis" for i in range(n_components)],
    )
    return pca, pcs


def apply_pca(pca, df, basis_cols):
    """Applique le PCA sur tout le dataset."""
    X = df[basis_cols].copy()
    mask = X.notna().all(axis=1)
    scores = pca.transform(X.loc[mask])
    pcs = pd.DataFrame(
        scores,
        index=X.loc[mask].index,
        columns=[f"PC{i+1}_basis" for i in range(pca.n_components_)],
    )
    return pcs


@st.cache_data
def cached_pca(df_train, basis_cols):
    return fit_pca_on_basis(df_train, basis_cols, n_components=2)


# ============================================================
#                 FEATURE BUILDER
# ============================================================

def build_features_for_tenor(df, tenor: str) -> pd.DataFrame:
    """
    Construit les features pour un tenor donnÃ© :
      - USDSWAP_T, EURSWAP_T, spread_T
      - slopes 10Yâ€“2Y (USD/EUR)
      - PC1_basis, PC2_basis
      - EURUSD
      - d_basis_5d, vol_basis_60d
    """
    target_col = f"EUBASIS_{tenor}"
    if target_col not in df.columns:
        raise ValueError(f"Colonne {target_col} manquante dans le CSV.")

    usd_col = f"USDSWAP_{tenor}"
    eur_col = f"EURSWAP_{tenor}"
    if usd_col not in df.columns or eur_col not in df.columns:
        raise ValueError(f"Colonnes {usd_col} ou {eur_col} manquantes dans le CSV.")

    data = df.copy()

    # Spread local
    data["spread_T"] = data[usd_col] - data[eur_col]

    # Slopes 10Yâ€“2Y (on suppose que ces colonnes existent)
    required_slopes = ["USDSWAP_10Y", "USDSWAP_2Y", "EURSWAP_10Y", "EURSWAP_2Y"]
    for col in required_slopes:
        if col not in data.columns:
            raise ValueError(f"Colonne {col} manquante pour calculer les slopes 10Yâ€“2Y.")

    data["slope_usd_10y_2y"] = data["USDSWAP_10Y"] - data["USDSWAP_2Y"]
    data["slope_eur_10y_2y"] = data["EURSWAP_10Y"] - data["EURSWAP_2Y"]

    # Dynamiques du basis
    data["d_basis_5d"] = data[target_col].diff(5)
    data["vol_basis_60d"] = data[target_col].rolling(60).std()

    # PCA factors
    for col in ["PC1_basis", "PC2_basis"]:
        if col not in data.columns:
            raise ValueError(f"{col} manquant dans le DataFrame (PCA).")

    # FX
    if "EURUSD" not in data.columns:
        raise ValueError("Colonne EURUSD manquante dans le CSV.")

    FEATURES = [
        usd_col,
        eur_col,
        "spread_T",
        "slope_usd_10y_2y",
        "slope_eur_10y_2y",
        "PC1_basis",
        "PC2_basis",
        "EURUSD",
        "d_basis_5d",
        "vol_basis_60d",
    ]

    out = data[FEATURES + [target_col]].dropna()
    out = out.rename(columns={target_col: "target_basis"})
    return out


# ============================================================
#            LGBM + GRID SEARCH (LIGHT) + SHAP
# ============================================================

def tune_lgbm_hyperparams(X: pd.DataFrame, y: pd.Series):
    """
    GridSearch "light" sur les 4 paramÃ¨tres principaux :
      - learning_rate
      - num_leaves
      - max_depth
      - n_estimators
    2x2x2x2 = 16 combinaisons â†’ rapide.
    """
    if X.shape[0] < 100:
        st.warning("Pas assez de donnÃ©es pour un GridSearch sÃ©rieux, on utilise des paramÃ¨tres par dÃ©faut.")
        default_params = {
            "learning_rate": 0.03,
            "num_leaves": 63,
            "max_depth": -1,
            "n_estimators": 400,
            "subsample": 0.9,
            "colsample_bytree": 0.9,
            "min_child_samples": 20,
        }
        return default_params, pd.DataFrame()

    param_grid = {
        "learning_rate": [0.01, 0.03],
        "num_leaves": [31, 63],
        "max_depth": [-1, 8],
        "n_estimators": [300, 600],
    }

    base_model = LGBMRegressor(
        subsample=0.9,
        colsample_bytree=0.9,
        min_child_samples=20,
        random_state=42,
        n_jobs=-1,
    )

    tscv = TimeSeriesSplit(n_splits=2)

    grid = GridSearchCV(
        estimator=base_model,
        param_grid=param_grid,
        cv=tscv,
        scoring="neg_mean_squared_error",
        n_jobs=-1,
        verbose=0,
        return_train_score=True,
    )

    grid.fit(X, y)

    cv_results = pd.DataFrame(grid.cv_results_)
    cv_results["MSE"] = -cv_results["mean_test_score"]
    cv_results = cv_results.sort_values("MSE").reset_index(drop=True)

    best_params = grid.best_params_.copy()
    best_params["subsample"] = 0.9
    best_params["colsample_bytree"] = 0.9
    best_params["min_child_samples"] = 20

    # SÃ©curitÃ© : ne JAMAIS laisser n_jobs traÃ®ner dans best_params
    if "n_jobs" in best_params:
        best_params.pop("n_jobs")

    return best_params, cv_results


def train_lgbm_and_shap(X: pd.DataFrame, y: pd.Series):
    """
    Pipeline :
      1) GridSearch hyperparams
      2) LGBM FULL (toutes features) pour SHAP
      3) SHAP sur un Ã©chantillon (max 800 points)
      4) SÃ©lection de features (95% masse SHAP)
      5) LGBM FINAL rÃ©-entraÃ®nÃ© sur les features sÃ©lectionnÃ©es
    """
    if X.shape[0] < 50:
        raise ValueError("Pas assez de points pour LGBM + SHAP.")

    # 1) Hyperparams
    best_params, cv_full = tune_lgbm_hyperparams(X, y)

    # 2) ModÃ¨le FULL
    if "n_jobs" in best_params:
        best_params.pop("n_jobs")

    model_full = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
    )
    model_full.fit(X, y)

    # 3) SHAP sur un sample
    explainer = shap.TreeExplainer(model_full)
    X_shap = X.sample(min(800, len(X)), random_state=42)
    shap_values = explainer.shap_values(X_shap)

    shap_abs = np.abs(shap_values).mean(axis=0)
    shap_importances = pd.Series(shap_abs, index=X.columns).sort_values(ascending=False)

    # 4) SÃ©lection features jusqu'Ã  95% masse SHAP
    total = shap_importances.sum()
    cum = 0.0
    keep_cols = []
    for col, val in shap_importances.items():
        keep_cols.append(col)
        cum += val
        if cum / total >= 0.95:
            break

    # 5) ModÃ¨le FINAL sur keep_cols uniquement
    if "n_jobs" in best_params:
        best_params.pop("n_jobs")

    model_final = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
    )
    model_final.fit(X[keep_cols], y)

    return model_final, keep_cols, shap_importances, best_params, cv_full


@st.cache_resource
def cached_lgbm_train(X: pd.DataFrame, y: pd.Series):
    """Cache le training LGBM+Grid+SHAP pour (X,y)."""
    return train_lgbm_and_shap(X, y)


# ============================================================
#                    STREAMLIT APP
# ============================================================

def main():
    st.title("ðŸ“ˆ EURUSD Basis â€“ Fair Value Calibration (LGBM + PCA + SHAP)")

    uploaded = st.file_uploader("ðŸ“„ Upload main CSV (multi-asset)", type=["csv"])
    if uploaded is None:
        st.stop()

    # --- Lecture
    df = pd.read_csv(uploaded)
    if "date" not in df.columns:
        st.error("Le CSV doit contenir une colonne 'date'.")
        st.stop()

    df["date"] = pd.to_datetime(df["date"], format="%d/%m/%Y")
    df = df.set_index("date").sort_index()

    # --- Basis columns
    basis_cols = [c for c in df.columns if c.startswith("EUBASIS_")]
    if not basis_cols:
        st.error("Aucune colonne EUBASIS_ trouvÃ©e dans le CSV.")
        st.stop()

    # --- Split temporel
    st.markdown("### ðŸ”€ Train / Test Split")
    split_pct = st.slider("Pourcentage In-sample", min_value=60, max_value=95, value=80)

    N = len(df)
    split_idx = int(N * split_pct / 100)
    split_idx = max(1, min(split_idx, N - 1))

    df_train = df.iloc[:split_idx].copy()
    df_test = df.iloc[split_idx:].copy()

    st.info(
        f"Train : {df_train.index[0].date()} â†’ {df_train.index[-1].date()}\n"
        f"Test  : {df_test.index[0].date()} â†’ {df_test.index[-1].date()}"
    )

    # --- PCA sur basis
    pca, pcs_train = cached_pca(df_train, basis_cols)
    pcs_full = apply_pca(pca, df, basis_cols)
    df = df.join(pcs_full, how="left")

    # --- Tenors
    tenors_available = sorted([c.replace("EUBASIS_", "") for c in basis_cols])
    sel_tenors = st.multiselect(
        "Tenors Ã  calibrer",
        tenors_available,
        default=[t for t in ["1Y", "2Y", "5Y"] if t in tenors_available] or tenors_available,
    )
    if not sel_tenors:
        st.warning("SÃ©lectionne au moins un tenor.")
        st.stop()

    # --- Bouton Run
    if "run_calib" not in st.session_state:
        st.session_state["run_calib"] = False

    if st.button("ðŸš€ Lancer / Relancer la calibration"):
        st.session_state["run_calib"] = True

    if not st.session_state["run_calib"]:
        st.stop()

    # --- Boucle tenors
    status = st.empty()
    progress = st.progress(0.0)

    results = {}
    best_params_by_tenor = {}
    features_by_tenor = {}
    grids_by_tenor = {}

    for i, tenor in enumerate(sel_tenors, start=1):
        status.markdown(f"**Calibration tenor `{tenor}` ({i}/{len(sel_tenors)})â€¦**")
        progress.progress(i / len(sel_tenors))

        try:
            feat_all = build_features_for_tenor(df, tenor)
        except Exception as e:
            st.warning(f"{tenor} ignorÃ© : {e}")
            continue

        # alignement train via intersection d'index
        common_idx_train = feat_all.index.intersection(df_train.index)
        feat_train = feat_all.loc[common_idx_train].copy()

        if feat_train.shape[0] < 80:
            st.warning(f"{tenor} : moins de 80 points en train, ignorÃ©.")
            continue

        X_tr = feat_train.drop(columns=["target_basis"])
        y_tr = feat_train["target_basis"]

        try:
            model, keep_cols, shap_imp, best_params, cv_table = cached_lgbm_train(X_tr, y_tr)
        except Exception as e:
            st.warning(f"{tenor} : erreur LGBM/SHAP : {e}")
            continue

        best_params_by_tenor[tenor] = best_params
        features_by_tenor[tenor] = keep_cols
        grids_by_tenor[tenor] = cv_table

        # FULL sÃ©rie de prediction sur les mÃªmes features
        X_all = feat_all[keep_cols]
        y_all = feat_all["target_basis"]
        y_hat_all = model.predict(X_all)

        df_res = pd.DataFrame(
            {"basis": y_all, "fair": y_hat_all},
            index=feat_all.index,
        ).sort_index()

        results[tenor] = df_res

    if not results:
        st.error("Aucun tenor calibrÃ©.")
        st.stop()

    # ============================================================
    #         VISU + METRICS
    # ============================================================

    st.markdown("---")
    st.header("ðŸ“ˆ Basis vs Fair Value â€“ Train & Test (OOS en gris)")

    tenor_plot = st.selectbox("Tenor Ã  afficher", list(results.keys()))
    dfp = results[tenor_plot]

    # Intersections d'index pour Ã©viter KeyError
    idx_is = dfp.index.intersection(df_train.index)
    idx_os = dfp.index.intersection(df_test.index)

    df_is = dfp.loc[idx_is].dropna()
    df_os = dfp.loc[idx_os].dropna()

    if df_is.empty or df_os.empty:
        st.warning("Pas assez de points non-NaN en train ou test pour ce tenor.")
    else:
        def compute_metrics(a, b):
            return {
                "RMSE": float(np.sqrt(mean_squared_error(a, b))),
                "MAE": float(mean_absolute_error(a, b)),
                "IC": float(np.corrcoef(a, b)[0, 1]),
            }

        met_is = compute_metrics(df_is["basis"], df_is["fair"])
        met_os = compute_metrics(df_os["basis"], df_os["fair"])

        st.subheader("MÃ©triques In-sample vs Out-of-sample")
        st.dataframe(pd.DataFrame({"In-sample": met_is, "Out-of-sample": met_os}))

        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(dfp.index, dfp["basis"], label="Basis")
        ax.plot(dfp.index, dfp["fair"], label="Fair Value", linestyle="--")

        # zone test
        ax.axvspan(
            df_test.index.min(),
            df_test.index.max(),
            color="lightgray",
            alpha=0.3,
            label="Test period",
        )

        ax.set_title(f"Basis vs Fair Value â€“ {tenor_plot}")
        ax.grid(True)
        ax.legend()
        st.pyplot(fig)

    # ============================================================
    #         TABLEAUX GLOBAUX
    # ============================================================

    st.markdown("---")
    st.header("ðŸ“Š RÃ©sumÃ© global par tenor")

    if best_params_by_tenor:
        st.subheader("Best hyperparams par tenor")
        st.dataframe(pd.DataFrame(best_params_by_tenor).T)

    if features_by_tenor:
        st.subheader("Features retenues (SHAP) par tenor")
        all_feats = sorted({f for lst in features_by_tenor.values() for f in lst})
        mat = pd.DataFrame(False, index=all_feats, columns=features_by_tenor.keys())
        for t, cols in features_by_tenor.items():
            mat.loc[cols, t] = True
        st.dataframe(mat)

    if grids_by_tenor:
        st.subheader("RÃ©sultats GridSearch (MSE triÃ©) par tenor")
        for t, tab in grids_by_tenor.items():
            st.markdown(f"### {t}")
            st.dataframe(tab)


if __name__ == "__main__":
    main()
