import numpy as np
import pandas as pd
import streamlit as st

from sklearn.decomposition import PCA
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error

from lightgbm import LGBMRegressor
import shap
import matplotlib.pyplot as plt


# ============================================================
#          PCA SUR LES RETURNS DU BASIS UNIQUEMENT
# ============================================================

def fit_pca_on_basis_returns(df: pd.DataFrame, basis_cols, n_components=2):
    """
    PCA sur les RETURNS 1D du basis:
    X_ret = EUBASIS_*.diff(1)
    On retourne un PCA et les scores in-sample (pour info).
    """
    X_ret = df[basis_cols].diff(1).dropna(how="any")
    if X_ret.empty:
        raise ValueError("Pas assez de donnÃ©es pour PCA returns (tout est NaN).")

    pca = PCA(n_components=n_components, random_state=42)
    scores = pca.fit_transform(X_ret.values)
    pcs = pd.DataFrame(
        scores,
        index=X_ret.index,
        columns=[f"PC{i+1}_basis" for i in range(n_components)],
    )
    return pca, pcs


def apply_pca_on_basis_returns(pca: PCA, df: pd.DataFrame, basis_cols):
    """
    Applique la PCA returns au dataset complet :
    X_ret = EUBASIS_*.diff(1), puis transform, puis alignement sur df.index.
    """
    X_ret = df[basis_cols].diff(1)
    mask = X_ret.notna().all(axis=1)

    if mask.sum() == 0:
        raise ValueError("Aucun point valide pour PCA returns sur l'ensemble du dataset.")

    scores = pca.transform(X_ret.loc[mask].values)
    pcs = pd.DataFrame(
        scores,
        index=X_ret.loc[mask].index,
        columns=[f"PC{i+1}_basis" for i in range(pca.n_components_)],
    )

    # On rÃ©aligne sur l'index complet de df (NaN lÃ  oÃ¹ on n'a pas de returns)
    pcs_full = pcs.reindex(df.index)
    return pcs_full


@st.cache_data
def cached_pca_returns(df_train: pd.DataFrame, basis_cols):
    """Cache PCA returns in-sample."""
    return fit_pca_on_basis_returns(df_train, basis_cols, n_components=2)


# ============================================================
#                 FEATURE BUILDER (MODE D + VIX)
# ============================================================

def build_features_for_tenor(df: pd.DataFrame, tenor: str) -> pd.DataFrame:
    """
    Feature Engine COMPLET (MODE D), avec PCA sur returns uniquement:
      - Levels: USDSWAP_T, EURSWAP_T, spread_T
      - Slopes 10Yâ€“2Y (USD/EUR)
      - Returns (1d, 5d) sur USD/EUR/Basis
      - VolatilitÃ© basis (20d, 60d)
      - PCA returns du basis: PC1_basis, PC2_basis
      - FX EURUSD (niveau + return)
      - VIX (niveau + return + vol)
      - Momentum basis (5d / 20d)
    """

    target_col = f"EUBASIS_{tenor}"
    if target_col not in df.columns:
        raise ValueError(f"Colonne {target_col} manquante dans le CSV.")

    usd_col = f"USDSWAP_{tenor}"
    eur_col = f"EURSWAP_{tenor}"
    if usd_col not in df.columns or eur_col not in df.columns:
        raise ValueError(f"Colonnes {usd_col} / {eur_col} manquantes dans le CSV.")

    data = df.copy()

    # 1) Levels
    data["spread_T"] = data[usd_col] - data[eur_col]

    # 2) Slopes 10Yâ€“2Y
    required_slopes = ["USDSWAP_10Y", "USDSWAP_2Y", "EURSWAP_10Y", "EURSWAP_2Y"]
    for col in required_slopes:
        if col not in data.columns:
            raise ValueError(f"Colonne {col} manquante pour calculer les slopes 10Yâ€“2Y.")
    data["slope_usd_10y_2y"] = data["USDSWAP_10Y"] - data["USDSWAP_2Y"]
    data["slope_eur_10y_2y"] = data["EURSWAP_10Y"] - data["EURSWAP_2Y"]

    # 3) Returns 1d / 5d
    data["d_usd_1d"] = data[usd_col].diff(1)
    data["d_eur_1d"] = data[eur_col].diff(1)
    data["d_basis_1d"] = data[target_col].diff(1)

    data["d_usd_5d"] = data[usd_col].diff(5)
    data["d_eur_5d"] = data[eur_col].diff(5)
    data["d_basis_5d"] = data[target_col].diff(5)

    # 4) VolatilitÃ©s basis
    data["vol_basis_20d"] = data[target_col].diff(1).rolling(20).std()
    data["vol_basis_60d"] = data[target_col].diff(1).rolling(60).std()

    # 5) PCA returns (PC1_basis, PC2_basis) dÃ©jÃ  calculÃ©e via cached_pca_returns
    for col in ["PC1_basis", "PC2_basis"]:
        if col not in data.columns:
            raise ValueError(f"{col} manquant (PCA sur returns du basis).")

    # 6) FX EURUSD
    if "EURUSD" not in data.columns:
        raise ValueError("Colonne EURUSD manquante.")
    data["d_EURUSD_1d"] = data["EURUSD"].diff(1)

    # 7) VIX (niveau + returns + vol)
    if "VIX" in data.columns:
        data["d_VIX_1d"] = data["VIX"].diff(1)
        data["vol_VIX_20d"] = data["VIX"].diff(1).rolling(20).std()
    else:
        st.warning("VIX non prÃ©sent dans le CSV, on met des zÃ©ros.")
        data["VIX"] = 0.0
        data["d_VIX_1d"] = 0.0
        data["vol_VIX_20d"] = 0.0

    # 8) Momentum basis
    data["mom_basis_5_20"] = data[target_col].diff(5) / data[target_col].diff(20)

    FEATURES = [
        # Levels
        usd_col, eur_col, "spread_T",

        # Slopes
        "slope_usd_10y_2y", "slope_eur_10y_2y",

        # Returns
        "d_usd_1d", "d_eur_1d", "d_basis_1d",
        "d_usd_5d", "d_eur_5d", "d_basis_5d",

        # Vol basis
        "vol_basis_20d", "vol_basis_60d",

        # PCA returns basis
        "PC1_basis", "PC2_basis",

        # FX
        "EURUSD", "d_EURUSD_1d",

        # VIX
        "VIX", "d_VIX_1d", "vol_VIX_20d",

        # Momentum
        "mom_basis_5_20",
    ]

    out = data[FEATURES + [target_col]].dropna()
    out = out.rename(columns={target_col: "target_basis"})
    return out


# ============================================================
#            LGBM + GRID SEARCH (LIGHT) + SHAP
# ============================================================

def tune_lgbm_hyperparams(X: pd.DataFrame, y: pd.Series):
    """
    GridSearch lÃ©ger sur 4 hyperparams clefs.
    """
    if X.shape[0] < 100:
        st.warning("Pas assez de donnÃ©es pour un GridSearch sÃ©rieux, on utilise des paramÃ¨tres par dÃ©faut.")
        default_params = {
            "learning_rate": 0.03,
            "num_leaves": 63,
            "max_depth": -1,
            "n_estimators": 400,
            "subsample": 0.9,
            "colsample_bytree": 0.9,
            "min_child_samples": 20,
        }
        return default_params, pd.DataFrame()

    param_grid = {
        "learning_rate": [0.01, 0.03],
        "num_leaves": [31, 63],
        "max_depth": [-1, 8],
        "n_estimators": [300, 600],
    }

    base_model = LGBMRegressor(
        subsample=0.9,
        colsample_bytree=0.9,
        min_child_samples=20,
        random_state=42,
        n_jobs=-1,
    )

    # tu peux monter Ã  3 splits pour plus de robustesse
    tscv = TimeSeriesSplit(n_splits=3)

    grid = GridSearchCV(
        estimator=base_model,
        param_grid=param_grid,
        cv=tscv,
        scoring="neg_mean_squared_error",
        n_jobs=-1,
        verbose=0,
        return_train_score=True,
    )

    grid.fit(X, y)

    cv_results = pd.DataFrame(grid.cv_results_)
    cv_results["MSE"] = -cv_results["mean_test_score"]
    cv_results = cv_results.sort_values("MSE").reset_index(drop=True)

    best_params = grid.best_params_.copy()
    best_params["subsample"] = 0.9
    best_params["colsample_bytree"] = 0.9
    best_params["min_child_samples"] = 20

    if "n_jobs" in best_params:
        best_params.pop("n_jobs")

    return best_params, cv_results


def train_lgbm_and_shap(X: pd.DataFrame, y: pd.Series):
    """
    1) GridSearch hyperparams
    2) LGBM FULL pour SHAP
    3) SHAP sur un Ã©chantillon
    4) SÃ©lection features (95% masse)
    5) LGBM FINAL sur features sÃ©lectionnÃ©es
    """
    if X.shape[0] < 50:
        raise ValueError("Pas assez de points pour LGBM + SHAP.")

    best_params, cv_full = tune_lgbm_hyperparams(X, y)

    if "n_jobs" in best_params:
        best_params.pop("n_jobs")

    model_full = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
    )
    model_full.fit(X, y)

    explainer = shap.TreeExplainer(model_full)
    X_shap = X.sample(min(800, len(X)), random_state=42)
    shap_values = explainer.shap_values(X_shap)

    shap_abs = np.abs(shap_values).mean(axis=0)
    shap_importances = pd.Series(shap_abs, index=X.columns).sort_values(ascending=False)

    total = shap_importances.sum()
    cum = 0.0
    keep_cols = []
    for col, val in shap_importances.items():
        keep_cols.append(col)
        cum += val
        if cum / total >= 0.95:
            break

    if "n_jobs" in best_params:
        best_params.pop("n_jobs")

    model_final = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
    )
    model_final.fit(X[keep_cols], y)

    return model_final, keep_cols, shap_importances, best_params, cv_full


@st.cache_resource
def cached_lgbm_train(X: pd.DataFrame, y: pd.Series):
    return train_lgbm_and_shap(X, y)


# ============================================================
#                    STREAMLIT APP
# ============================================================

def main():
    st.title("ðŸ“ˆ EURUSD Basis â€“ Fair Value (PCA RETURNS + LGBM + SHAP)")

    uploaded = st.file_uploader("ðŸ“„ Upload main CSV (multi-asset)", type=["csv"])
    if uploaded is None:
        st.stop()

    df = pd.read_csv(uploaded)
    if "date" not in df.columns:
        st.error("Le CSV doit contenir une colonne 'date'.")
        st.stop()

    df["date"] = pd.to_datetime(df["date"], format="%d/%m/%Y")
    df = df.set_index("date").sort_index()

    basis_cols = [c for c in df.columns if c.startswith("EUBASIS_")]
    if not basis_cols:
        st.error("Aucune colonne EUBASIS_ trouvÃ©e dans le CSV.")
        st.stop()

    # ========= Split Train/Test
    st.markdown("### ðŸ”€ Train / Test Split")
    split_pct = st.slider("Pourcentage In-sample", 60, 95, 80)

    N = len(df)
    split_idx = int(N * split_pct / 100)
    split_idx = max(1, min(split_idx, N - 1))

    df_train = df.iloc[:split_idx].copy()
    df_test = df.iloc[split_idx:].copy()

    st.info(
        f"Train : {df_train.index[0].date()} â†’ {df_train.index[-1].date()}\n"
        f"Test  : {df_test.index[0].date()} â†’ {df_test.index[-1].date()}"
    )

    # ========= PCA RETURNS sur basis
    pca_ret, pcs_train = cached_pca_returns(df_train, basis_cols)
    pcs_full = apply_pca_on_basis_returns(pca_ret, df, basis_cols)
    df = df.join(pcs_full, how="left")

    # ========= Choix tenors
    tenors_available = sorted([c.replace("EUBASIS_", "") for c in basis_cols])
    sel_tenors = st.multiselect(
        "Tenors Ã  calibrer",
        tenors_available,
        default=[t for t in ["1Y", "2Y", "5Y"] if t in tenors_available] or tenors_available,
    )
    if not sel_tenors:
        st.warning("SÃ©lectionne au moins un tenor.")
        st.stop()

    # ========= Bouton Run
    if "run_calib" not in st.session_state:
        st.session_state["run_calib"] = False

    if st.button("ðŸš€ Lancer / Relancer la calibration"):
        st.session_state["run_calib"] = True

    if not st.session_state["run_calib"]:
        st.stop()

    # ========= Boucle tenors
    status = st.empty()
    progress = st.progress(0.0)

    results = {}
    best_params_by_tenor = {}
    features_by_tenor = {}
    grids_by_tenor = {}

    for i, tenor in enumerate(sel_tenors, start=1):
        status.markdown(f"**Calibration tenor `{tenor}` ({i}/{len(sel_tenors)})â€¦**")
        progress.progress(i / len(sel_tenors))

        try:
            feat_all = build_features_for_tenor(df, tenor)
        except Exception as e:
            st.warning(f"{tenor} ignorÃ© : {e}")
            continue

        # alignement train
        common_idx_train = feat_all.index.intersection(df_train.index)
        feat_train = feat_all.loc[common_idx_train].copy()

        if feat_train.shape[0] < 80:
            st.warning(f"{tenor} : moins de 80 points en train, ignorÃ©.")
            continue

        X_tr = feat_train.drop(columns=["target_basis"])
        y_tr = feat_train["target_basis"]

        try:
            model, keep_cols, shap_imp, best_params, cv_table = cached_lgbm_train(X_tr, y_tr)
        except Exception as e:
            st.warning(f"{tenor} : erreur LGBM/SHAP : {e}")
            continue

        best_params_by_tenor[tenor] = best_params
        features_by_tenor[tenor] = keep_cols
        grids_by_tenor[tenor] = cv_table

        X_all = feat_all[keep_cols]
        y_all = feat_all["target_basis"]
        y_hat_all = model.predict(X_all)

        df_res = pd.DataFrame(
            {"basis": y_all, "fair": y_hat_all},
            index=feat_all.index,
        ).sort_index()

        results[tenor] = df_res

    if not results:
        st.error("Aucun tenor calibrÃ©.")
        return

    # ========= VISU + METRICS
    st.markdown("---")
    st.header("ðŸ“ˆ Basis vs Fair Value â€“ Train & Test (OOS en gris)")

    tenor_plot = st.selectbox("Tenor Ã  afficher", list(results.keys()))
    dfp = results[tenor_plot]

    idx_is = dfp.index.intersection(df_train.index)
    idx_os = dfp.index.intersection(df_test.index)

    df_is = dfp.loc[idx_is].dropna()
    df_os = dfp.loc[idx_os].dropna()

    if df_is.empty or df_os.empty:
        st.warning("Pas assez de points non-NaN en train ou test pour ce tenor.")
    else:
        def compute_metrics(a, b):
            return {
                "RMSE": float(np.sqrt(mean_squared_error(a, b))),
                "MAE": float(mean_absolute_error(a, b)),
                "IC": float(np.corrcoef(a, b)[0, 1]),
            }

        met_is = compute_metrics(df_is["basis"], df_is["fair"])
        met_os = compute_metrics(df_os["basis"], df_os["fair"])

        st.subheader("MÃ©triques In-sample vs Out-of-sample")
        st.dataframe(pd.DataFrame({"In-sample": met_is, "Out-of-sample": met_os}))

        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(dfp.index, dfp["basis"], label="Basis")
        ax.plot(dfp.index, dfp["fair"], label="Fair Value", linestyle="--")

        ax.axvspan(
            df_test.index.min(),
            df_test.index.max(),
            color="lightgray",
            alpha=0.3,
            label="Test period",
        )

        ax.set_title(f"Basis vs Fair Value â€“ {tenor_plot}")
        ax.grid(True)
        ax.legend()
        st.pyplot(fig)

    # ========= TABLEAUX GLOBAUX
    st.markdown("---")
    st.header("ðŸ“Š RÃ©sumÃ© global par tenor")

    if best_params_by_tenor:
        st.subheader("Best hyperparams par tenor")
        st.dataframe(pd.DataFrame(best_params_by_tenor).T)

    if features_by_tenor:
        st.subheader("Features retenues (SHAP) par tenor")
        all_feats = sorted({f for lst in features_by_tenor.values() for f in lst})
        mat = pd.DataFrame(False, index=all_feats, columns=features_by_tenor.keys())
        for t, cols in features_by_tenor.items():
            mat.loc[cols, t] = True
        st.dataframe(mat)

    if grids_by_tenor:
        st.subheader("RÃ©sultats GridSearch (MSE triÃ©) par tenor")
        for t, tab in grids_by_tenor.items():
            st.markdown(f"### {t}")
            st.dataframe(tab)


if __name__ == "__main__":
    main()
