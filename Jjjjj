# ============================================================
#   EURUSD BASIS ‚Äì Curve PCA + Fair Value LGBM + HMM + Forecast
#   3 tabs:
#     1) Curve PCA
#     2) Fair Value + HMM + Trade Blotter
#     3) Forecast LGBM on Z-score
#   Version sans MOM features
# ============================================================

import numpy as np
import pandas as pd
import streamlit as st

from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.decomposition import PCA
from lightgbm import LGBMRegressor
import shap
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from hmmlearn import hmm
import json
import os


# ============================================================
# 0) PAGE CONFIG ‚Äì LARGE LAYOUT
# ============================================================

st.set_page_config(
    page_title="EURUSD Basis ‚Äì Curve PCA & Fair Value",
    layout="wide",
    initial_sidebar_state="expanded",
)

HYPERPARAMS_FILE = "lgbm_best_params_per_tenor.json"


# ============================================================
# 1) UTILS G√âN√âRIQUES
# ============================================================

def load_saved_hyperparams():
    if os.path.exists(HYPERPARAMS_FILE):
        with open(HYPERPARAMS_FILE, "r") as f:
            return json.load(f)
    return {}


def make_base_params(saved_params=None):
    base = dict(
        learning_rate=0.03,
        num_leaves=31,
        max_depth=4,
        n_estimators=300,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_samples=20,
    )
    if saved_params is not None:
        base.update(saved_params)

    base.update(
        reg_lambda=5.0,
        reg_alpha=1.0,
        random_state=42,
        n_jobs=-1,
    )
    return base


def train_lgbm_shap_select(X, y, base_params):
    """
    1) Entra√Æne un LGBM sur tous les features
    2) Calcule SHAP global
    3) Garde ~95% de la masse SHAP (+ PC1/PC2 si pr√©sents)
    4) Refit un mod√®le final sur ce sous-ensemble.
    """
    model_full = LGBMRegressor(**base_params)
    model_full.fit(X, y)

    explainer = shap.TreeExplainer(model_full)
    Xs = X.sample(min(800, len(X)), random_state=42)
    shap_vals = explainer.shap_values(Xs)

    shap_abs = np.abs(shap_vals).mean(axis=0)
    shap_importances = pd.Series(shap_abs, index=X.columns).sort_values(ascending=False)

    total = shap_importances.sum()
    cum = 0.0
    keep = []
    for col, v in shap_importances.items():
        keep.append(col)
        cum += v
        if cum / total >= 0.95:
            break

    for m in ["PC1_basis", "PC2_basis"]:
        if m in X.columns and m not in keep:
            keep.append(m)

    model_final = LGBMRegressor(**base_params)
    model_final.fit(X[keep], y)

    return model_final, keep, shap_importances


def plot_zscore_with_hmm_background(dfp, regimes, tenor_label):
    """
    dfp : DataFrame avec 'zscore'
    regimes : Series index√©e par date avec √©tats HMM (0,1,2‚Ä¶)
    """
    z = dfp["zscore"].dropna()
    reg = regimes.reindex(z.index).dropna()

    if z.empty or reg.empty:
        st.warning("Not enough common data for z-score + HMM plot.")
        return

    common = z.index.intersection(reg.index)
    z = z.reindex(common)
    reg = reg.reindex(common)

    fig, ax = plt.subplots(figsize=(12, 3))
    ax.plot(z.index, z.values, label="Z-score", linewidth=1.4)

    unique_states = sorted(reg.unique())
    colors = ["#e0f3db", "#fee0b6", "#fbb4ae", "#c6dbef", "#f2f2f2"]
    cmap = {s: colors[i % len(colors)] for i, s in enumerate(unique_states)}

    current = reg.iloc[0]
    start = reg.index[0]

    for t, s in zip(reg.index[1:], reg.iloc[1:]):
        if s != current:
            ax.axvspan(start, t, color=cmap[current], alpha=0.25)
            start = t
            current = s
    ax.axvspan(start, reg.index[-1], color=cmap[current], alpha=0.25)

    ax.axhline(0, color="grey", linestyle="--")
    ax.grid(True)
    ax.set_title(f"{tenor_label} ‚Äì Z-score with HMM regimes")
    ax.set_ylabel("z-score")

    patches = [mpatches.Patch(color=cmap[s], label=f"Regime {s}") for s in unique_states]
    ax.legend(handles=[ax.lines[0]] + patches, loc="upper left", fontsize=8)

    st.pyplot(fig)


# ============================================================
# 2) OUTILS PCA COURBE (TAB 1)
# ============================================================

def tenor_to_years(col_name: str) -> float:
    """
    Convertit un nom de colonne style XXX_3M / 6M / 1Y / 2Y / 5Y / 10Y / 30Y en maturit√© en ann√©es.
    """
    t = col_name.split("_")[-1].upper()
    if t.endswith("M"):
        n = float(t.replace("M", ""))
        return n / 12.0
    if t.endswith("Y"):
        n = float(t.replace("Y", ""))
        return n
    try:
        return float(t)
    except Exception:
        return np.nan


def build_curve_dict(df: pd.DataFrame):
    """
    Construit un dict {nom_de_courbe: DataFrame} √† partir d'un multi_asset_full,
    en groupant par pr√©fixe avant le premier underscore.
    Exemple : EUBASIS_..., EURSWAP_..., USDSWAP_..., EUINF_...
    """
    curves = {}
    cols = [c for c in df.columns if "_" in c]
    prefixes = sorted(set(c.split("_")[0] for c in cols))

    for pref in prefixes:
        curve_cols = [c for c in cols if c.startswith(pref + "_")]
        if not curve_cols:
            continue
        sub = df[curve_cols].dropna()
        if sub.empty:
            continue

        # Ordonner les colonnes par maturit√© en ann√©es
        mats = [tenor_to_years(c) for c in curve_cols]
        order = np.argsort(mats)
        ordered_cols = [curve_cols[i] for i in order]
        curves[pref] = sub[ordered_cols]

    return curves


def run_curve_pca(df_curve: pd.DataFrame, n_components: int = 3):
    """
    PCA sur une courbe (colonnes = tenors).
    Retourne un dict avec :
      - pca
      - data (df nettoy√©)
      - columns
      - maturities
      - eigenvectors
      - explained_variance (cumul√©e)
      - resid_std (std des r√©sidus par tenor)
    """
    data = df_curve.dropna()
    if data.empty or data.shape[1] < 2:
        raise ValueError("Not enough data or tenors to run PCA on this curve.")

    X = data.values
    k = min(n_components, X.shape[1])
    pca = PCA(n_components=k)
    pca.fit(X)

    # Reconstructions & r√©sidus pour estimer la std des r√©sidus par tenor
    X_recon = pca.inverse_transform(pca.transform(X))
    resid = X - X_recon
    resid_std = resid.std(axis=0, ddof=1)

    maturities = [tenor_to_years(c) for c in data.columns]

    return {
        "pca": pca,
        "data": data,
        "columns": list(data.columns),
        "maturities": maturities,
        "eigenvectors": pca.components_,
        "explained_variance": pca.explained_variance_ratio_.sum(),
        "resid_std": resid_std,
    }


def reconstruct_one_date(model: dict, date):
    """
    Reconstruit la courbe PCA pour une date donn√©e.
    Retourne un DataFrame avec Actual / Reconstructed / Residual / Residual_z.
    """
    data = model["data"]
    if date not in data.index:
        raise ValueError("Selected date not available for this curve.")

    x0 = data.loc[date].values.reshape(1, -1)
    pca = model["pca"]
    X_col = model["columns"]
    resid_std = model["resid_std"]

    scores = pca.transform(x0)
    x_recon = pca.inverse_transform(scores)[0]
    residual = x0[0] - x_recon

    # z-score des r√©sidus par tenor
    resid_z = residual / np.where(resid_std == 0, np.nan, resid_std)

    out = pd.DataFrame(
        {
            "Actual": x0[0],
            "Reconstructed": x_recon,
            "Residual": residual,
            "Residual_z": resid_z,
        },
        index=X_col,
    )
    return out, scores[0]


# ============================================================
# 3) MAIN APP (3 TABS)
# ============================================================

def main():
    st.title("üìà EURUSD Basis ‚Äì Curve PCA & Fair Value (LGBM + HMM + Forecast)")

    tab1, tab2, tab3 = st.tabs([
        "1Ô∏è‚É£ Curve PCA",
        "2Ô∏è‚É£ Fair Value + HMM",
        "3Ô∏è‚É£ Forecast ‚Äì LGBM on Z-Score"
    ])

    # =======================================================
    # TAB 1 ‚Äì Curve PCA
    # =======================================================
    with tab1:
        st.header("1Ô∏è‚É£ Curve PCA ‚Äî Analyse d‚Äôune courbe (rates / basis / inflation)")

        multi_file = st.file_uploader(
            "Upload multi_asset_full.csv (courbes swaps / basis / inflation)",
            type=["csv"],
            key="multi_asset_file",
        )

        if multi_file is None:
            st.info("Upload multi_asset_full.csv pour utiliser la PCA de courbe.")
        else:
            try:
                multi = pd.read_csv(multi_file)
                if "date" in multi.columns:
                    multi["date"] = pd.to_datetime(multi["date"], format="%d/%m/%Y", errors="coerce")
                    multi = multi.dropna(subset=["date"]).set_index("date").sort_index()
            except Exception as e:
                st.error(f"Erreur de chargement de multi_asset_full.csv : {e}")
                st.stop()

            curves = build_curve_dict(multi)
            if not curves:
                st.warning("Aucune courbe trouv√©e dans multi_asset_full.csv.")
            else:
                curve_name = st.selectbox("Courbe √† analyser", list(curves.keys()))
                df_curve = curves[curve_name]

                st.markdown(f"**Courbe s√©lectionn√©e :** `{curve_name}`")
                st.dataframe(df_curve.tail(), use_container_width=True)

                try:
                    model_tmp = run_curve_pca(df_curve)
                except ValueError as e:
                    st.error(str(e))
                else:
                    dates_available = model_tmp["data"].index
                    selected_date = st.selectbox("Date √† analyser", dates_available[::-1])

                    # On rerun pour √™tre s√ªr d'avoir un mod√®le clean (m√™me data)
                    model = run_curve_pca(df_curve)
                    table, factors = reconstruct_one_date(model, selected_date)

                    st.subheader(f"PCA Reconstruction ‚Äî {curve_name} ‚Äî {selected_date.date()}")
                    st.dataframe(
                        table.style.background_gradient(cmap="coolwarm", subset=["Residual_z"]),
                        use_container_width=True,
                    )

                    maturities = model["maturities"]
                    EV = model["eigenvectors"]

                    # Loadings (factors)
                    st.subheader("PCA Loadings (Level / Slope / Curvature)")
                    fig1, ax1 = plt.subplots(figsize=(8, 3))
                    for i in range(len(EV)):
                        ax1.plot(maturities, EV[i], marker="o", label=f"Factor {i+1}")
                    ax1.grid(True)
                    ax1.set_xlabel("Maturity (years)")
                    ax1.set_ylabel("Loading")
                    ax1.legend()
                    st.pyplot(fig1)

                    # Actual vs Reconstruction
                    st.subheader("Actual vs PCA Reconstruction")
                    fig2, ax2 = plt.subplots(figsize=(8, 3))
                    ax2.plot(maturities, table["Actual"], marker="o", label="Actual")
                    ax2.plot(maturities, table["Reconstructed"], marker="x", label="Reconstructed")
                    ax2.grid(True)
                    ax2.set_xlabel("Maturity (years)")
                    ax2.set_ylabel("Rate / Basis (bp)")
                    ax2.legend()
                    st.pyplot(fig2)

                    # Residual Z-scores
                    st.subheader("Residual Z-Scores")
                    fig3, ax3 = plt.subplots(figsize=(8, 2.5))
                    ax3.bar(table.index, table["Residual_z"])
                    ax3.axhline(0, linestyle="--", color="grey")
                    ax3.grid(True, axis="y")
                    plt.xticks(rotation=45)
                    st.pyplot(fig3)

                    # Auto PM pitch
                    st.subheader("üé§ Auto PM Pitch")
                    max_resid = table["Residual_z"].abs().idxmax()
                    z = table.loc[max_resid, "Residual_z"]
                    expl_var = model["explained_variance"]

                    pitch = f"""
For **{curve_name}**, the first PCA factors explain about **{expl_var:.2%}**
of curve variance (classic level / slope / curvature structure).

On **{selected_date.date()}**, the **{max_resid}** point shows the largest
dislocation with a z-score of **{z:.2f}**, i.e. a statistically significant
anomaly vs the PCA curve.

These curvature-driven deviations tend to mean-revert and often provide
clean RV opportunities (receive when too wide, pay when too tight).
"""
                    st.write(pitch)

    # =======================================================
    # TAB 2 ‚Äì Fair Value + HMM + TRADE blotter
    # =======================================================
    with tab2:
        st.header("2Ô∏è‚É£ Fair Value ‚Äì LGBM + HMM-integrated Signal")

        data_file = st.file_uploader(
            "Upload data_with_features.csv (basis + features + PC1_basis, etc.)",
            type=["csv"],
            key="features_file",
        )
        if data_file is None:
            st.stop()

        df = pd.read_csv(data_file)
        if "date" not in df.columns:
            st.error("Missing 'date' column in data_with_features.csv.")
            st.stop()

        df["date"] = pd.to_datetime(df["date"], format="%d/%m/%Y", errors="coerce")
        df = df.dropna(subset=["date"])
        df = df.set_index("date").sort_index()
        df = df.loc[:, ~df.columns.str.startswith("Unnamed")]

        saved_hparams = load_saved_hyperparams()

        # -------- Initialisation du cache --------
        if "results" not in st.session_state:
            st.session_state["results"] = None
            st.session_state["regimes"] = None
            st.session_state["trans_mat"] = None
            st.session_state["reg_descr_map"] = None
            st.session_state["stress_regime"] = None

        # -------- Fen√™tre de dates --------
        min_d = df.index.min().date()
        max_d = df.index.max().date()

        col1, col2 = st.columns(2)
        with col1:
            start = st.date_input("Start date", min_d)
        with col2:
            end = st.date_input("End date", max_d)

        if start > end:
            st.error("Start date > End date.")
            st.stop()

        dfw = df.loc[str(start):str(end)]
        if dfw.empty:
            st.error("Empty window after date filter.")
            st.stop()

        st.info(f"Window: {dfw.index[0].date()} ‚Üí {dfw.index[-1].date()} ({len(dfw)} points)")

        # -------- Tenors --------
        basis_cols = [c for c in dfw.columns if c.startswith("EUBASIS_")]
        if not basis_cols:
            st.error("No EUBASIS_ columns in data_with_features.csv.")
            st.stop()

        tenors = sorted(c.replace("EUBASIS_", "") for c in basis_cols)

        sel_tenors = st.multiselect(
            "Select tenors",
            tenors,
            default=["1Y", "2Y", "5Y", "10Y"] if set(["1Y", "2Y", "5Y", "10Y"]).issubset(tenors) else tenors,
        )
        if not sel_tenors:
            st.stop()

        use_saved = st.checkbox("Use saved hyperparameters (if available)", True)
        run_clicked = st.button("üöÄ Run model (LGBM + HMM)")

        # ------------- 1) CALIBRATION + HMM (bouton) -------------
        if run_clicked:
            results = {}
            models = {}
            features_all = {}

            macro_cols = [
                "US_2s10s", "DE_2s10s", "POLICY_DIFF", "CREDIT_US",
                "CREDIT_EUR_US", "ESTR_FED", "USGG10YR_ret_1d", "GDBR10_ret_1d",
            ]

            for tenor in sel_tenors:
                target = f"TARGET_RV_{tenor}"
                basis_col = f"EUBASIS_{tenor}"
                usd = f"USDSWAP_{tenor}"
                eur = f"EURSWAP_{tenor}"
                spread = f"SPREAD_{tenor}"
                d1 = f"D_BASIS_1D_{tenor}"
                d5 = f"D_BASIS_5D_{tenor}"
                vol = f"VOL20_{tenor}"
                ma60 = f"BASIS_MA60_{tenor}"

                required = [
                    target, basis_col, usd, eur, spread, d1, d5, vol, ma60,
                    "PC1_basis", "PC2_basis", "EURUSD", "D_EURUSD_1D", "VIX", "MOVE"
                ]

                missing = [c for c in required if c not in dfw.columns]
                if missing:
                    continue

                available_macro = [c for c in macro_cols if c in dfw.columns]

                data2 = dfw[required + available_macro].dropna()
                if len(data2) < 100:
                    continue

                features = [
                    usd, eur, spread, d1, d5, vol,
                    "PC1_basis", "PC2_basis",
                    "EURUSD", "D_EURUSD_1D", "VIX", "MOVE"
                ] + available_macro

                X = data2[features]
                y = data2[target]

                if use_saved and tenor in saved_hparams:
                    params = make_base_params(saved_hparams[tenor])
                else:
                    params = make_base_params()

                model, keep, _ = train_lgbm_shap_select(X, y, params)
                models[tenor] = model
                features_all[tenor] = keep

                X_use = X[keep]
                y_hat = model.predict(X_use)

                fair = data2[ma60] + y_hat
                resid = data2[basis_col] - fair
                vol_resid = resid.rolling(60).std().fillna(method="bfill")
                z = resid / vol_resid.replace(0, np.nan)

                dfp = pd.DataFrame(
                    {
                        "basis": data2[basis_col],
                        "fair": fair,
                        "residual": resid,
                        "vol_resid_60d": vol_resid,
                        "zscore": z,
                    },
                    index=data2.index,
                )
                results[tenor] = dfp

            # -------- HMM sur macro --------
            regime_feats = [c for c in [
                "PC1_basis", "VIX", "MOVE", "US_2s10s", "DE_2s10s", "CREDIT_US", "POLICY_DIFF"
            ] if c in dfw.columns]

            reg_df = dfw[regime_feats].dropna()
            use_hmm_local = True

            if len(reg_df) < 200 or len(regime_feats) < 2:
                use_hmm_local = False
                regimes = None
                trans_mat = None
                reg_descr_map = {}
                stress_regime = None
            else:
                hmm_model = hmm.GaussianHMM(
                    n_components=3,
                    covariance_type="full",
                    n_iter=200,
                    random_state=42
                )
                hmm_model.fit(reg_df.values)
                states = hmm_model.predict(reg_df.values)
                regimes = pd.Series(states, index=reg_df.index, name="regime")
                trans_mat = hmm_model.transmat_
                n_states = trans_mat.shape[0]

                # D√©terminer le r√©gime "stress" via VIX ou MOVE
                vix_means = {}
                for k in range(n_states):
                    idx_k = states == k
                    sub_k = reg_df.iloc[idx_k]
                    if "VIX" in reg_df.columns:
                        vix_means[k] = sub_k["VIX"].mean()
                    elif "MOVE" in reg_df.columns:
                        vix_means[k] = sub_k["MOVE"].mean()
                    else:
                        vix_means[k] = 0.0

                stress_regime = max(vix_means, key=vix_means.get)

                # Label qualitatif par ordre de stress
                sorted_by_stress = sorted(vix_means.items(), key=lambda x: x[1])
                low_reg = sorted_by_stress[0][0]
                high_reg = sorted_by_stress[-1][0]

                reg_descr_map = {}
                for k in range(n_states):
                    if k == low_reg:
                        reg_descr_map[k] = "Low vol / risk-on / carry-friendly"
                    elif k == high_reg:
                        reg_descr_map[k] = "High stress / risk-off / USD liq. stress"
                    else:
                        reg_descr_map[k] = "Transition / rates & inflation repricing"

            # -------- Sauvegarde dans session_state --------
            st.session_state["results"] = results
            st.session_state["regimes"] = regimes
            st.session_state["trans_mat"] = trans_mat
            st.session_state["reg_descr_map"] = reg_descr_map
            st.session_state["stress_regime"] = stress_regime

        # ------------- 2) LECTURE DU CACHE & AFFICHAGE -------------
        results = st.session_state["results"]
        regimes = st.session_state["regimes"]
        trans_mat = st.session_state["trans_mat"]
        reg_descr_map = st.session_state["reg_descr_map"]
        stress_regime = st.session_state["stress_regime"]

        if results is None or len(results) == 0:
            st.warning("Click on 'üöÄ Run model' to calibrate and display results.")
            st.stop()

        # ============================================================
        # MASTER TABLE: SIGNAL + HMM
        # ============================================================

        rows_master = []
        use_hmm = regimes is not None and trans_mat is not None and stress_regime is not None

        for tenor, dfp in results.items():
            dfp_nonan = dfp.dropna(subset=["zscore", "residual"])
            if dfp_nonan.empty:
                continue

            last = dfp_nonan.iloc[-1]
            last_date = last.name
            z = float(last["zscore"])
            resid = float(last["residual"])

            regime_col = None
            regime_descr = ""
            p_stay = np.nan
            p_to_stress = np.nan
            z_eff = z
            trade_flag = "NO"

            if use_hmm:
                reg_sub = regimes.loc[:last_date]
                if not reg_sub.empty:
                    reg_curr = int(reg_sub.iloc[-1])
                    regime_col = reg_curr
                    regime_descr = reg_descr_map.get(reg_curr, "")

                    p_stay = float(trans_mat[reg_curr, reg_curr])
                    if stress_regime is not None and stress_regime != reg_curr:
                        p_to_stress = float(trans_mat[reg_curr, stress_regime])
                    else:
                        p_to_stress = 0.0

                    # Z-score effectif pond√©r√© par persistance & risque de stress
                    z_eff = z * p_stay * (1.0 - p_to_stress)

                    # R√®gle simple de trade
                    threshold = 1.5
                    trade_flag = "YES" if abs(z_eff) > threshold else "NO"
                else:
                    trade_flag = "NO"
            else:
                threshold = 1.5
                trade_flag = "YES" if abs(z) > threshold else "NO"

            rows_master.append(
                {
                    "Tenor": tenor,
                    "Last date": last_date.date(),
                    "Last z": z,
                    "Residual (bp)": resid,
                    "Regime": regime_col,
                    "Regime descr": regime_descr,
                    "p_stay": p_stay,
                    "p_to_stress": p_to_stress,
                    "z_eff": z_eff,
                    "Trade?": trade_flag,
                }
            )

        master_table = pd.DataFrame(rows_master).set_index("Tenor")

        st.subheader("üß© Master Table ‚Äì Signal + HMM-integrated View")
        st.dataframe(master_table.round(3), use_container_width=True)

        # ------------------------------------------------------------
        #  AUTO COMMENTAIRE ‚Äì TRADE BLOTTER
        # ------------------------------------------------------------

        st.subheader("üìù Trade Blotter ‚Äì Auto Commentary")

        def format_line(row):
            tenor = row.name
            z_eff = row["z_eff"]
            z_raw = row["Last z"]
            resid_bp = row["Residual (bp)"]
            trade_flag = row["Trade?"]
            reg = row["Regime"]
            descr = row["Regime descr"]
            p_stay = row["p_stay"]
            p_to_stress = row["p_to_stress"]

            if np.isnan(z_eff):
                return f"- {tenor}: insufficient data."

            direction = "LONG basis" if z_eff < 0 else "SHORT basis"
            bias = "No-trade" if trade_flag == "NO" else direction

            line = (
                f"- **{tenor}**: z = {z_raw:.2f}, z_eff = {z_eff:.2f}, resid = {resid_bp:.1f} bp ‚Üí **{bias}**. "
            )
            if reg is not None:
                line += f"(Regime {reg}: {descr}, p_stay={p_stay:.2f}, p‚Üístress={p_to_stress:.2f})"
            return line

        for idx, row in master_table.iterrows():
            st.markdown(format_line(row))

        # ============================================================
        # GRAPHS ‚Äì BASIS VS FAIR & Z-SCORE (COMPACT VIEW)
        # ============================================================

        st.markdown("---")
        st.subheader("üìà Drill-down Graphs")

        tenor_sel = st.selectbox("Select tenor for graphs", list(results.keys()))
        dfp_sel = results[tenor_sel]

        colg1, colg2 = st.columns(2)

        with colg1:
            st.markdown(f"**{tenor_sel} ‚Äì Basis vs Fair Value**")
            fig1, ax1 = plt.subplots(figsize=(6, 3))
            ax1.plot(dfp_sel.index, dfp_sel["basis"], label="Basis")
            ax1.plot(dfp_sel.index, dfp_sel["fair"], label="Fair Value", linestyle="--")
            ax1.grid(True)
            ax1.legend(fontsize=8)
            st.pyplot(fig1)

        with colg2:
            st.markdown(f"**{tenor_sel} ‚Äì Z-score**")
            fig2, ax2 = plt.subplots(figsize=(6, 3))
            ax2.plot(dfp_sel.index, dfp_sel["zscore"])
            ax2.axhline(0, color="grey")
            ax2.axhline(1.5, linestyle="--", color="red")
            ax2.axhline(-1.5, linestyle="--", color="red")
            ax2.grid(True)
            st.pyplot(fig2)

        # ============================================================
        # MINI HMM BLOCK ‚Äì EN EXPANDER (COMPACT)
        # ============================================================

        st.markdown("---")
        with st.expander("üìâ HMM Regime details (macro context)", expanded=False):
            if not use_hmm:
                st.warning("HMM not available (not enough data/features).")
            else:
                st.markdown("**HMM Regimes Time Series**")
                fig_r, ax_r = plt.subplots(figsize=(12, 2.8))
                ax_r.plot(regimes.index, regimes.values, drawstyle="steps-post")
                ax_r.set_yticks(sorted(regimes.unique()))
                ax_r.set_yticklabels([f"Regime {k}" for k in sorted(regimes.unique())])
                ax_r.grid(True)
                st.pyplot(fig_r)

                st.markdown(f"**{tenor_sel} ‚Äì Z-score with HMM background**")
                plot_zscore_with_hmm_background(dfp_sel, regimes, tenor_sel)

                st.markdown("**HMM Transition Matrix**")
                n_states = trans_mat.shape[0]
                trans_df = pd.DataFrame(
                    trans_mat,
                    index=[f"Regime {k}" for k in range(n_states)],
                    columns=[f"Regime {k}" for k in range(n_states)],
                )
                st.dataframe(trans_df.style.format("{:.2f}"), use_container_width=True)

    # ============================================================
    # TAB 3 ‚Äì FORECASTING LGBM SUR Z-SCORE
    # ============================================================

    with tab3:
        st.header("3Ô∏è‚É£ Forecasting ‚Äî LightGBM on Basis Z-Score")

        if st.session_state.get("results") is None:
            st.warning("Run the model in Tab 2 before accessing the forecasting module.")
            st.stop()

        results = st.session_state["results"]

        tenor_fore = st.selectbox(
            "Select tenor for forecasting",
            list(results.keys())
        )

        horizon = st.slider("Forecast horizon (days)", 1, 10, 5)

        dfp_f = results[tenor_fore].dropna(subset=["zscore", "residual", "basis"]).copy()
        dfp_f["z_future"] = dfp_f["zscore"].shift(-horizon)
        df_f = dfp_f.dropna(subset=["z_future"])

        if len(df_f) < 80:
            st.warning(f"Not enough data for forecast on {tenor_fore} (need at least 80 points).")
            st.stop()

        # ---- Training ----
        Xf = df_f[["zscore", "residual", "basis"]]
        yf = df_f["z_future"]

        split = int(0.8 * len(df_f))
        X_train, X_test = Xf.iloc[:split], Xf.iloc[split:]
        y_train, y_test = yf.iloc[:split], yf.iloc[split:]

        fore_model = LGBMRegressor(
            n_estimators=300,
            max_depth=3,
            learning_rate=0.05,
            subsample=0.9,
            colsample_bytree=0.9,
            random_state=42,
        )
        fore_model.fit(X_train, y_train)

        y_pred = fore_model.predict(X_test)

        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)

        st.markdown(
            f"**{tenor_fore} ‚Äì {horizon}-day z-score forecast**  \n"
            f"- RMSE (test) = **{rmse:.2f}**  \n"
            f"- MAE (test) = **{mae:.2f}**"
        )

        # ---- Forecast pour la derni√®re date ----
        last_X = dfp_f[["zscore", "residual", "basis"]].iloc[[-1]]
        z_future_pred = fore_model.predict(last_X)[0]

        st.subheader("üìç Latest Forecast")
        st.markdown(
            f"In **{horizon} days**, expected z-score = **{z_future_pred:.2f}** "
            f"on **{tenor_fore}**."
        )

        # ---- Graph Forecast vs Realised ----
        st.markdown("üìà Forecast vs Realised (Test Set)")
        figf, axf = plt.subplots(figsize=(12, 4))
        axf.plot(y_test.index, y_test.values, label="Realised z(t+H)")
        axf.plot(y_test.index, y_pred, label="Forecast", linestyle="--")
        axf.grid(True)
        axf.legend()
        st.pyplot(figf)


if __name__ == "__main__":
    main()
