import numpy as np
import pandas as pd
import streamlit as st

from sklearn.decomposition import PCA
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error

from lightgbm import LGBMRegressor
import shap
import matplotlib.pyplot as plt


# ============================================================
# 1) PCA SUR LES RETURNS DU BASIS
# ============================================================

def compute_pca_returns(df_train: pd.DataFrame, basis_cols, n_components=2):
    """
    PCA sur les returns 1D du basis (train uniquement).
    Returns: pca object + scores train (PC1, PC2...).
    """
    R_train = df_train[basis_cols].diff(1).fillna(0.0)
    pca = PCA(n_components=n_components, random_state=42)
    scores = pca.fit_transform(R_train.values)

    pcs_train = pd.DataFrame(
        scores,
        index=df_train.index,
        columns=[f"PC{i+1}_basis" for i in range(n_components)]
    )
    return pca, pcs_train


def apply_pca_to_full_df(pca: PCA, df: pd.DataFrame, basis_cols):
    """
    Applique la PCA returns sur l'ensemble du dataset (mÃªme transformation).
    """
    R_full = df[basis_cols].diff(1).fillna(0.0)
    scores_full = pca.transform(R_full.values)

    pcs_full = pd.DataFrame(
        scores_full,
        index=df.index,
        columns=[f"PC{i+1}_basis" for i in range(scores_full.shape[1])]
    )
    return pcs_full


# ============================================================
# 2) FEATURE BUILDER MICRO + MACRO LIGHT
# ============================================================

def build_features_for_tenor(df: pd.DataFrame, tenor: str, use_macro: bool = True):
    """
    Construit les features pour un tenor :
      - Micro : swaps, basis, slopes, returns, vol, momentum
      - PCA returns : PC1_basis, PC2_basis
      - Macro light : quelques facteurs structurels seulement (macro ON/OFF)
    """

    target_col = f"EUBASIS_{tenor}"
    usd_col = f"USDSWAP_{tenor}"
    eur_col = f"EURSWAP_{tenor}"

    if target_col not in df.columns:
        raise ValueError(f"{target_col} missing in DataFrame")

    data = df.copy()

    # ----- MICRO : swaps / basis -----
    data["spread_T"] = data[usd_col] - data[eur_col]
    data["slope_usd"] = data["USDSWAP_10Y"] - data["USDSWAP_2Y"]
    data["slope_eur"] = data["EURSWAP_10Y"] - data["EURSWAP_2Y"]

    data["d_basis_1d"] = data[target_col].diff(1).fillna(0.0)
    data["d_basis_5d"] = data[target_col].diff(5).fillna(0.0)

    ret_basis = data[target_col].diff(1).fillna(0.0)
    data["vol_20d"] = ret_basis.rolling(20).std().fillna(0.0)

    data["mom_5_20"] = (
        data[target_col].diff(5) / data[target_col].diff(20)
    ).replace([np.inf, -np.inf], 0.0).fillna(0.0)

    # ----- PCA returns -----
    if "PC1_basis" not in data.columns or "PC2_basis" not in data.columns:
        raise ValueError("PC1_basis / PC2_basis manquants dans df (PCA non join).")

    # ----- FX / VIX / MOVE minimal -----
    if "EURUSD" in data.columns:
        data["d_EURUSD_1d"] = data["EURUSD"].diff().fillna(0.0)
    else:
        data["d_EURUSD_1d"] = 0.0

    if "VIX" not in data.columns:
        data["VIX"] = 0.0
    if "MOVE" not in data.columns:
        data["MOVE"] = 0.0

    # Base minimal micro features
    base_features = [
        usd_col, eur_col, "spread_T",
        "slope_usd", "slope_eur",
        "d_basis_1d", "d_basis_5d",
        "vol_20d", "mom_5_20",
        "PC1_basis", "PC2_basis",
        "EURUSD", "d_EURUSD_1d",
        "VIX", "MOVE",
    ]

    # ----- Macro light : quelques drivers structurels seulement -----
    macro_candidate = [
        "US_2s10s",
        "DE_2s10s",
        "POLICY_DIFF",
        "CREDIT_US",
        "CREDIT_EUR_US",
        "ESTR_FED",
        "USGG10YR_ret_1d",
        "GDBR10_ret_1d",
    ]
    macro_features = [c for c in macro_candidate if c in data.columns]

    if use_macro:
        FEATURES = base_features + macro_features
    else:
        FEATURES = base_features

    out = data[FEATURES + [target_col]].rename(columns={target_col: "target_basis"})
    out = out.dropna(subset=["target_basis"])
    feat_cols = [c for c in out.columns if c != "target_basis"]
    out[feat_cols] = out[feat_cols].fillna(0.0)

    return out, FEATURES, macro_features


# ============================================================
# 3) LGBM + GRIDSEARCH LIGHT + SHAP
# ============================================================

def run_gridsearch_light(X: pd.DataFrame, y: pd.Series):
    """
    GridSearch trÃ¨s light pour Ã©viter l'overfit :
    - profondeur limitÃ©e
    - peu de num_leaves
    - peu de n_estimators
    """

    param_grid = {
        "learning_rate": [0.03],
        "num_leaves": [15, 31],
        "max_depth": [3, 4],
        "n_estimators": [150, 300],
    }

    base = LGBMRegressor(
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_samples=20,
        random_state=42,
        n_jobs=-1,
    )

    cv = TimeSeriesSplit(n_splits=3)

    grid = GridSearchCV(
        base,
        param_grid,
        scoring="neg_root_mean_squared_error",
        cv=cv,
        n_jobs=-1,
        verbose=0,
    )

    grid.fit(X, y)

    res = pd.DataFrame(grid.cv_results_)
    res["RMSE"] = -res["mean_test_score"]
    res = res.sort_values("RMSE").reset_index(drop=True)

    best = grid.best_params_
    # On garde des hyperparams modestes
    best["subsample"] = 0.8
    best["colsample_bytree"] = 0.8
    best["min_child_samples"] = 20

    return best, res


def train_lgbm_with_shap_light(X: pd.DataFrame, y: pd.Series):
    """
    - GridSearch lÃ©ger
    - SHAP pour importance
    - SÃ©lection 95% masse + mandatory features
    - Refit final
    """

    best_params, cv_table = run_gridsearch_light(X, y)

    model_full = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
        reg_lambda=5.0,
        reg_alpha=1.0,
    )
    model_full.fit(X, y)

    explainer = shap.TreeExplainer(model_full)
    Xs = X.sample(min(800, len(X)), random_state=42)
    shap_vals = explainer.shap_values(Xs)

    shap_abs = np.abs(shap_vals).mean(axis=0)
    shap_importances = pd.Series(shap_abs, index=X.columns).sort_values(ascending=False)

    total = shap_importances.sum()
    cum = 0.0
    keep_cols = []
    for col, val in shap_importances.items():
        keep_cols.append(col)
        cum += val
        if cum / total >= 0.95:
            break

    mandatory = ["PC1_basis", "PC2_basis", "d_basis_1d", "spread_T", "slope_usd", "slope_eur"]
    for m in mandatory:
        if m in X.columns and m not in keep_cols:
            keep_cols.append(m)

    model_final = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
        reg_lambda=5.0,
        reg_alpha=1.0,
    )
    model_final.fit(X[keep_cols], y)

    return model_final, keep_cols, shap_importances, best_params, cv_table


@st.cache_resource
def cached_lgbm_train(X, y):
    # cache par signature X, y
    return train_lgbm_with_shap_light(X, y)


# ============================================================
# 4) STREAMLIT APP
# ============================================================

def main():
    st.title("ðŸ“ˆ EURUSD Basis Fair Value â€“ Version stable (macro light)")

    uploaded = st.file_uploader("ðŸ“„ Upload multi_asset_full_with_macro.csv", type=["csv"])
    if uploaded is None:
        st.stop()

    df = pd.read_csv(uploaded)
    if "date" not in df.columns:
        st.error("Colonne 'date' manquante.")
        st.stop()

    df["date"] = pd.to_datetime(df["date"], dayfirst=True)
    df = df.set_index("date").sort_index()
    df = df.loc[:, ~df.columns.str.startswith("Unnamed")]

    # Basis cols
    basis_cols = [c for c in df.columns if c.startswith("EUBASIS_")]
    if not basis_cols:
        st.error("Aucune colonne EUBASIS_ trouvÃ©e.")
        st.stop()

    tenors = sorted([c.replace("EUBASIS_", "") for c in basis_cols])

    # Macro ON/OFF
    use_macro = st.checkbox("Inclure les features macro light", value=True)

    # Train/Test split
    split_pct = st.slider("Pourcentage In-sample", 60, 95, 80)
    N = len(df)
    k = int(N * split_pct / 100)
    df_train = df.iloc[:k]
    df_test = df.iloc[k:]

    st.info(
        f"Train : {df_train.index[0].date()} â†’ {df_train.index[-1].date()}  \n"
        f"Test  : {df_test.index[0].date()} â†’ {df_test.index[-1].date()}"
    )

    # PCA returns
    pca, pcs_train = compute_pca_returns(df_train, basis_cols)
    pcs_full = apply_pca_to_full_df(pca, df, basis_cols)
    df = df.join(pcs_full, how="left")

    # Tenors
    sel_tenors = st.multiselect("Tenors Ã  calibrer", tenors, default=["1Y", "2Y", "5Y", "10Y"])
    if not sel_tenors:
        st.stop()

    if st.button("ðŸš€ Lancer la calibration"):
        st.session_state["run"] = True
    if "run" not in st.session_state or not st.session_state["run"]:
        st.stop()

    results = {}
    best_params_all = {}
    features_all = {}

    for tenor in sel_tenors:
        st.subheader(f"Calibration {tenor} â€“ macro {'ON' if use_macro else 'OFF'}")

        try:
            feat_all, FEATURES, macro_feats = build_features_for_tenor(df, tenor, use_macro=use_macro)
        except Exception as e:
            st.warning(f"{tenor} ignorÃ© : {e}")
            continue

        idx_tr = feat_all.index.intersection(df_train.index)
        feat_tr = feat_all.loc[idx_tr]

        if feat_tr.shape[0] < 150:
            st.warning(f"{tenor} : train trop court ({feat_tr.shape[0]} points).")
            continue

        X_tr = feat_tr.drop(columns=["target_basis"])
        y_tr = feat_tr["target_basis"]

        model, keep_cols, shap_imp, best_params, cv_table = cached_lgbm_train(X_tr, y_tr)

        best_params_all[tenor] = best_params
        features_all[tenor] = keep_cols

        X_all = feat_all[keep_cols]
        y_all = feat_all["target_basis"]
        y_hat = model.predict(X_all)

        results[tenor] = pd.DataFrame({"basis": y_all, "fair": y_hat}, index=feat_all.index)

    if not results:
        st.error("Aucun tenor calibrÃ©.")
        st.stop()

    # --------- VISU ---------
    st.header("ðŸ“Š Basis vs Fair Value â€“ Train & Test")

    tenor_plot = st.selectbox("Tenor Ã  afficher", list(results.keys()))
    dfp = results[tenor_plot]

    idx_is = dfp.index.intersection(df_train.index)
    idx_os = dfp.index.intersection(df_test.index)
    df_is = dfp.loc[idx_is].dropna()
    df_os = dfp.loc[idx_os].dropna()

    def metrics(a, b):
        return {
            "RMSE": float(np.sqrt(mean_squared_error(a, b))),
            "MAE": float(mean_absolute_error(a, b)),
            "IC": float(np.corrcoef(a, b)[0, 1]),
        }

    st.subheader("MÃ©triques In-sample / Out-of-sample")
    met_is = metrics(df_is["basis"], df_is["fair"])
    met_os = metrics(df_os["basis"], df_os["fair"])
    st.dataframe(pd.DataFrame({"In-sample": met_is, "Out-of-sample": met_os}))

    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(dfp.index, dfp["basis"], label="Basis")
    ax.plot(dfp.index, dfp["fair"], label="Fair value", linestyle="--")
    ax.axvspan(df_test.index.min(), df_test.index.max(), color="lightgray", alpha=0.3, label="Test")
    ax.set_title(f"{tenor_plot} â€“ Macro {'ON' if use_macro else 'OFF'}")
    ax.grid(True)
    ax.legend()
    st.pyplot(fig)

    # --------- Hyperparams & Features ---------
    st.header("ðŸ”§ Hyperparams & Features (version light)")

    st.subheader("Best hyperparams par tenor")
    st.dataframe(pd.DataFrame(best_params_all).T)

    st.subheader("Features retenues (SHAP)")
    all_feats = sorted({f for cols in features_all.values() for f in cols})
    mat = pd.DataFrame(False, index=all_feats, columns=features_all.keys())
    for t, cols in features_all.items():
        mat.loc[cols, t] = True
    st.dataframe(mat)


if __name__ == "__main__":
    main()
