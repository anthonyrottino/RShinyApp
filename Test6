import numpy as np
import pandas as pd
import streamlit as st

from sklearn.decomposition import PCA
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error

from lightgbm import LGBMRegressor
import shap
import matplotlib.pyplot as plt


# ============================================================
# 0) FONCTION D'AJOUT DES FEATURES MACRO (BBG)
# ============================================================

def build_macro_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Enrichit le DataFrame avec des features macro construites Ã  partir des tickers :
    USGG2YR, USGG10YR, GDBR2, GDBR10, FEDL01, ECBDDEP, ESTRON, MOVE, VIX,
    CDXIG, CDXHY, ITRAXX_MAIN, GBPUSD, USDJPY, EURUSD.
    Ne fait PAS de dropna global : on laisse la gestion des NaN aux feature builders.
    """

    df = df.copy()

    # 1) Courbes 2s10s US & DE
    if {"USGG10YR", "USGG2YR"}.issubset(df.columns):
        df["US_2s10s"] = df["USGG10YR"] - df["USGG2YR"]
    if {"GDBR10", "GDBR2"}.issubset(df.columns):
        df["DE_2s10s"] = df["GDBR10"] - df["GDBR2"]

    # 2) DiffÃ©rentiel de politique monÃ©taire Fed - ECB
    if {"FEDL01", "ECBDDEP"}.issubset(df.columns):
        df["POLICY_DIFF"] = df["FEDL01"] - df["ECBDDEP"]

    # 3) ESTR - Fed (funding diff)
    if {"ESTRON", "FEDL01"}.issubset(df.columns):
        df["ESTR_FED"] = df["ESTRON"] - df["FEDL01"]

    # 4) VolatilitÃ© : VIX & MOVE
    if "VIX" in df.columns:
        df["d_VIX_1d"] = df["VIX"].diff()
        df["vol_VIX_20d"] = df["VIX"].rolling(20).std()
    if "MOVE" in df.columns:
        df["d_MOVE_1d"] = df["MOVE"].diff()
        df["vol_MOVE_20d"] = df["MOVE"].rolling(20).std()

    # 5) CrÃ©dit : CDX IG / HY / iTraxx Main
    if {"CDXIG", "CDXHY"}.issubset(df.columns):
        df["CREDIT_US"] = df["CDXHY"] - df["CDXIG"]
    if {"ITRAXX_MAIN", "CDXIG"}.issubset(df.columns):
        df["CREDIT_EUR_US"] = df["ITRAXX_MAIN"] - df["CDXIG"]

    # 6) FX returns
    fx_list = ["EURUSD", "GBPUSD", "USDJPY"]
    for fx in fx_list:
        if fx in df.columns:
            df[f"{fx}_ret_1d"] = df[fx].pct_change()
            df[f"{fx}_ret_5d"] = df[fx].pct_change(5)

    # 7) Returns & vol des taux souverains
    y_list = ["USGG2YR", "USGG10YR", "GDBR2", "GDBR10"]
    for y in y_list:
        if y in df.columns:
            df[f"{y}_ret_1d"] = df[y].diff()
            df[f"{y}_vol_20d"] = df[y].rolling(20).std()

    # Nettoyage basique (on ne drop pas les lignes, on laisse faire les builders)
    df = df.replace([np.inf, -np.inf], np.nan)

    return df


# ============================================================
# 1) PCA SUR LES RETURNS DU BASIS
# ============================================================

def compute_pca_returns(df_train: pd.DataFrame, basis_cols, n_components=2):
    """PCA sur les returns 1D du basis (train seulement)."""
    R_train = df_train[basis_cols].diff(1).fillna(0.0)

    pca = PCA(n_components=n_components, random_state=42)
    scores = pca.fit_transform(R_train.values)

    pcs_train = pd.DataFrame(
        scores,
        index=df_train.index,
        columns=[f"PC{i+1}_basis" for i in range(n_components)]
    )
    return pca, pcs_train


def apply_pca_to_full_df(pca: PCA, df: pd.DataFrame, basis_cols):
    """Applique la PCA returns sur tout le DataFrame."""
    R_full = df[basis_cols].diff(1).fillna(0.0)
    scores_full = pca.transform(R_full.values)

    pcs_full = pd.DataFrame(
        scores_full,
        index=df.index,
        columns=[f"PC{i+1}_basis" for i in range(scores_full.shape[1])]
    )
    return pcs_full


# ============================================================
# 2) FEATURE ENGINEERING MICRO + MACRO POUR UN TENOR
# ============================================================

def build_features_for_tenor(df: pd.DataFrame, tenor: str):
    """
    Construit les features micro + macro pour un tenor donnÃ©.
    - Basis & swaps (niveau, slope, returns, vol, momentum)
    - PCA returns (PC1, PC2)
    - FX & VIX & MOVE
    - Macro (US_2s10s, DE_2s10s, POLICY_DIFF, ESTR_FED, CREDIT_US, CREDIT_EUR_US, etc.)
    """

    target_col = f"EUBASIS_{tenor}"
    usd_col = f"USDSWAP_{tenor}"
    eur_col = f"EURSWAP_{tenor}"

    if target_col not in df.columns:
        raise ValueError(f"{target_col} missing in DataFrame")

    data = df.copy()

    # --------- MICRO : swaps / basis ----------
    data["spread_T"] = data[usd_col] - data[eur_col]
    data["slope_usd"] = data["USDSWAP_10Y"] - data["USDSWAP_2Y"]
    data["slope_eur"] = data["EURSWAP_10Y"] - data["EURSWAP_2Y"]

    data["d_usd_1d"] = data[usd_col].diff(1).fillna(0.0)
    data["d_eur_1d"] = data[eur_col].diff(1).fillna(0.0)
    data["d_basis_1d"] = data[target_col].diff(1).fillna(0.0)

    data["d_usd_5d"] = data[usd_col].diff(5).fillna(0.0)
    data["d_eur_5d"] = data[eur_col].diff(5).fillna(0.0)
    data["d_basis_5d"] = data[target_col].diff(5).fillna(0.0)

    ret_basis = data[target_col].diff(1).fillna(0.0)
    data["vol_20d"] = ret_basis.rolling(20).std().fillna(0.0)
    data["vol_60d"] = ret_basis.rolling(60).std().fillna(0.0)

    data["mom_5_20"] = (data[target_col].diff(5) / data[target_col].diff(20)) \
        .replace([np.inf, -np.inf], 0.0).fillna(0.0)

    # --------- PCA returns (doivent dÃ©jÃ  Ãªtre dans df) ----------
    if "PC1_basis" not in data.columns or "PC2_basis" not in data.columns:
        raise ValueError("PC1_basis / PC2_basis manquants dans df (PCA non join).")

    # --------- FX / VIX / MOVE (macro-light) ----------
    if "EURUSD" in data.columns:
        data["d_EURUSD_1d"] = data["EURUSD"].diff().fillna(0.0)
    else:
        data["d_EURUSD_1d"] = 0.0

    if "VIX" not in data.columns:
        data["VIX"] = 0.0
    data["d_VIX_1d"] = data.get("d_VIX_1d", data["VIX"].diff().fillna(0.0))
    data["vol_VIX_20d"] = data.get("vol_VIX_20d", data["VIX"].rolling(20).std().fillna(0.0))

    if "MOVE" not in data.columns:
        data["MOVE"] = 0.0
    data["d_MOVE_1d"] = data.get("d_MOVE_1d", data["MOVE"].diff().fillna(0.0))
    data["vol_MOVE_20d"] = data.get("vol_MOVE_20d", data["MOVE"].rolling(20).std().fillna(0.0))

    # --------- MACRO FEATURES (si dispo) ----------
    candidate_macro = [
        "US_2s10s", "DE_2s10s", "POLICY_DIFF", "ESTR_FED",
        "CREDIT_US", "CREDIT_EUR_US",
        "EURUSD_ret_1d", "EURUSD_ret_5d",
        "GBPUSD_ret_1d", "GBPUSD_ret_5d",
        "USDJPY_ret_1d", "USDJPY_ret_5d",
        "USGG2YR_ret_1d", "USGG10YR_ret_1d",
        "GDBR2_ret_1d", "GDBR10_ret_1d",
        "USGG2YR_vol_20d", "USGG10YR_vol_20d",
        "GDBR2_vol_20d", "GDBR10_vol_20d",
    ]
    macro_feats = [c for c in candidate_macro if c in data.columns]

    base_features = [
        usd_col, eur_col, "spread_T",
        "slope_usd", "slope_eur",
        "d_usd_1d", "d_eur_1d", "d_basis_1d",
        "d_usd_5d", "d_eur_5d", "d_basis_5d",
        "vol_20d", "vol_60d",
        "PC1_basis", "PC2_basis",
        "EURUSD", "d_EURUSD_1d",
        "VIX", "d_VIX_1d", "vol_VIX_20d",
        "MOVE", "d_MOVE_1d", "vol_MOVE_20d",
        "mom_5_20",
    ]

    FEATURES = base_features + macro_feats

    out = data[FEATURES + [target_col]].rename(columns={target_col: "target_basis"})
    out = out.dropna(subset=["target_basis"])

    feat_cols = [c for c in out.columns if c != "target_basis"]
    out[feat_cols] = out[feat_cols].fillna(0.0)

    return out, FEATURES


# ============================================================
# 3) LGBM + GRIDSEARCH + SHAP
# ============================================================

def run_gridsearch(X: pd.DataFrame, y: pd.Series):
    param_grid = {
        "learning_rate": [0.01, 0.03],
        "num_leaves": [31, 63],
        "max_depth": [-1, 8],
        "n_estimators": [300, 600],
    }

    base = LGBMRegressor(
        subsample=0.9,
        colsample_bytree=0.9,
        min_child_samples=20,
        random_state=42,
        n_jobs=-1,
    )

    cv = TimeSeriesSplit(n_splits=3)

    grid = GridSearchCV(
        base,
        param_grid,
        scoring="neg_root_mean_squared_error",
        cv=cv,
        n_jobs=-1,
        verbose=0,
    )

    grid.fit(X, y)

    res = pd.DataFrame(grid.cv_results_)
    res["RMSE"] = -res["mean_test_score"]
    res = res.sort_values("RMSE").reset_index(drop=True)

    best = grid.best_params_
    best["subsample"] = 0.8
    best["colsample_bytree"] = 0.8
    best["min_child_samples"] = 20

    return best, res


def train_lgbm_with_shap(X: pd.DataFrame, y: pd.Series):
    best_params, cv_table = run_gridsearch(X, y)

    model_full = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
        reg_lambda=5.0,
        reg_alpha=1.0,
    )
    model_full.fit(X, y)

    explainer = shap.TreeExplainer(model_full)
    Xs = X.sample(min(800, len(X)), random_state=42)
    shap_vals = explainer.shap_values(Xs)

    shap_abs = np.abs(shap_vals).mean(axis=0)
    shap_importances = pd.Series(shap_abs, index=X.columns).sort_values(ascending=False)

    total = shap_importances.sum()
    cum = 0.0
    keep_cols = []
    for col, val in shap_importances.items():
        keep_cols.append(col)
        cum += val
        if cum / total >= 0.95:
            break

    # On force quelques features structurantes
    mandatory = ["PC1_basis", "PC2_basis", "d_basis_1d", "spread_T", "slope_usd", "slope_eur"]
    for m in mandatory:
        if m in X.columns and m not in keep_cols:
            keep_cols.append(m)

    model_final = LGBMRegressor(
        **best_params,
        random_state=42,
        n_jobs=-1,
        reg_lambda=5.0,
        reg_alpha=1.0,
    )
    model_final.fit(X[keep_cols], y)

    return model_final, keep_cols, shap_importances, best_params, cv_table


@st.cache_resource
def cached_lgbm_train(X, y):
    return train_lgbm_with_shap(X, y)


# ============================================================
# 4) STREAMLIT APP
# ============================================================

def main():
    st.title("ðŸ“ˆ EURUSD Basis Fair Value â€“ Micro + Macro (PCA returns + LGBM + SHAP)")

    uploaded = st.file_uploader("ðŸ“„ Upload CSV multi-asset", type=["csv"])
    if uploaded is None:
        st.stop()

    df = pd.read_csv(uploaded)
    if "date" not in df.columns:
        st.error("La colonne 'date' est requise.")
        st.stop()

    df["date"] = pd.to_datetime(df["date"], dayfirst=True)
    df = df.set_index("date").sort_index()

    # Nettoyage de colonnes inutiles (type 'Unnamed: xx')
    df = df.loc[:, ~df.columns.str.startswith("Unnamed")]

    # Ajout des features macro globales
    df = build_macro_features(df)

    # Basis cols
    basis_cols = [c for c in df.columns if c.startswith("EUBASIS_")]
    if not basis_cols:
        st.error("Aucune colonne EUBASIS_ trouvÃ©e.")
        st.stop()

    tenors = sorted([c.replace("EUBASIS_", "") for c in basis_cols])

    # Train/Test split
    split_pct = st.slider("Pourcentage In-sample", 60, 95, 80)
    N = len(df)
    k = int(N * split_pct / 100)
    df_train = df.iloc[:k]
    df_test = df.iloc[k:]

    st.info(
        f"Train : {df_train.index[0].date()} â†’ {df_train.index[-1].date()}  \n"
        f"Test  : {df_test.index[0].date()} â†’ {df_test.index[-1].date()}"
    )

    # PCA returns
    pca, pcs_train = compute_pca_returns(df_train, basis_cols)
    pcs_full = apply_pca_to_full_df(pca, df, basis_cols)
    df = df.join(pcs_full, how="left")

    # Tenors Ã  calibrer
    sel_tenors = st.multiselect("Tenors", tenors, default=["1Y", "2Y", "5Y"])
    if not sel_tenors:
        st.stop()

    if st.button("ðŸš€ Run Calibration"):
        st.session_state["run"] = True
    if "run" not in st.session_state or not st.session_state["run"]:
        st.stop()

    results = {}
    best_params_all = {}
    features_all = {}

    for tenor in sel_tenors:
        st.subheader(f"Calibration {tenor}")
        try:
            feat_all, _ = build_features_for_tenor(df, tenor)
        except Exception as e:
            st.warning(f"{tenor} ignorÃ© : {e}")
            continue

        idx_tr = feat_all.index.intersection(df_train.index)
        feat_tr = feat_all.loc[idx_tr]

        if feat_tr.shape[0] < 100:
            st.warning(f"{tenor} : train trop court ({feat_tr.shape[0]} points).")
            continue

        X_tr = feat_tr.drop(columns=["target_basis"])
        y_tr = feat_tr["target_basis"]

        model, keep_cols, shap_imp, best_params, cv_table = cached_lgbm_train(X_tr, y_tr)

        best_params_all[tenor] = best_params
        features_all[tenor] = keep_cols

        X_all = feat_all[keep_cols]
        y_all = feat_all["target_basis"]
        y_hat = model.predict(X_all)

        results[tenor] = pd.DataFrame({"basis": y_all, "fair": y_hat}, index=feat_all.index)

    if not results:
        st.error("Aucun tenor calibrÃ©.")
        st.stop()

    # --------- VISU ---------
    st.header("ðŸ“Š Basis vs Fair Value â€“ Train & Test")

    tenor_plot = st.selectbox("Tenor Ã  afficher", list(results.keys()))
    dfp = results[tenor_plot]

    idx_is = dfp.index.intersection(df_train.index)
    idx_os = dfp.index.intersection(df_test.index)
    df_is = dfp.loc[idx_is].dropna()
    df_os = dfp.loc[idx_os].dropna()

    def metrics(a, b):
        return {
            "RMSE": float(np.sqrt(mean_squared_error(a, b))),
            "MAE": float(mean_absolute_error(a, b)),
            "IC": float(np.corrcoef(a, b)[0, 1]),
        }

    st.subheader("MÃ©triques In-sample / Out-of-sample")
    met_is = metrics(df_is["basis"], df_is["fair"])
    met_os = metrics(df_os["basis"], df_os["fair"])
    st.dataframe(pd.DataFrame({"In-sample": met_is, "Out-of-sample": met_os}))

    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(dfp.index, dfp["basis"], label="Basis")
    ax.plot(dfp.index, dfp["fair"], label="Fair Value", linestyle="--")
    ax.axvspan(df_test.index.min(), df_test.index.max(), color="lightgray", alpha=0.3, label="Test")
    ax.set_title(tenor_plot)
    ax.grid(True)
    ax.legend()
    st.pyplot(fig)

    # --------- Hyperparams & features ---------
    st.header("ðŸ”§ Hyperparams & Features")

    st.subheader("Best hyperparams par tenor")
    st.dataframe(pd.DataFrame(best_params_all).T)

    st.subheader("Features retenues (SHAP)")
    all_feats = sorted({f for cols in features_all.values() for f in cols})
    mat = pd.DataFrame(False, index=all_feats, columns=features_all.keys())
    for t, cols in features_all.items():
        mat.loc[cols, t] = True
    st.dataframe(mat)


if __name__ == "__main__":
    main()
